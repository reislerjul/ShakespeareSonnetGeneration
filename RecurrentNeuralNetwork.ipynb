{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function from set 6 to parse the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_observations(text):\n",
    "    # Convert text to dataset.\n",
    "    lines = [line.split() for line in text.split('\\n') if line.split()]\n",
    "\n",
    "    obs_counter = 0\n",
    "    obs = []\n",
    "    obs_map = {}\n",
    "\n",
    "    for line in lines:\n",
    "        obs_elem = []\n",
    "        \n",
    "        for word in line:\n",
    "            word = re.sub(r'[^\\w]', '', word).lower()\n",
    "            if word not in obs_map:\n",
    "                # Add unique words to the observations map.\n",
    "                obs_map[word] = obs_counter\n",
    "                obs_counter += 1\n",
    "            \n",
    "            # Add the encoded word.\n",
    "            obs_elem.append(obs_map[word])\n",
    "        \n",
    "        # Add the encoded sequence.\n",
    "        obs.append(obs_elem)\n",
    "\n",
    "    return obs, obs_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(os.path.join(os.getcwd(), \\\n",
    "                         'Release/data/shakespeare.txt')).read()\n",
    "obs, obs_map = parse_observations(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has %d chars, %d unique (98029, 71)\n"
     ]
    }
   ],
   "source": [
    "all_chars = list(text)\n",
    "chars = sorted(list(set(text)))\n",
    "data_size, vocab_size = len(text), len(chars)\n",
    "print ('data has %d chars, %d unique' ,(data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_to_index = {char:idx for idx, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_char = {idx:char for idx, char in enumerate(chars)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: ',', 7: '-', 8: '.', 9: '0', 10: '1', 11: '2', 12: '3', 13: '4', 14: '5', 15: '6', 16: '7', 17: '8', 18: '9', 19: ':', 20: ';', 21: '?', 22: 'A', 23: 'B', 24: 'C', 25: 'D', 26: 'E', 27: 'F', 28: 'G', 29: 'H', 30: 'I', 31: 'J', 32: 'K', 33: 'L', 34: 'M', 35: 'N', 36: 'O', 37: 'P', 38: 'R', 39: 'S', 40: 'T', 41: 'U', 42: 'V', 43: 'W', 44: 'Y', 45: 'a', 46: 'b', 47: 'c', 48: 'd', 49: 'e', 50: 'f', 51: 'g', 52: 'h', 53: 'i', 54: 'j', 55: 'k', 56: 'l', 57: 'm', 58: 'n', 59: 'o', 60: 'p', 61: 'q', 62: 'r', 63: 's', 64: 't', 65: 'u', 66: 'v', 67: 'w', 68: 'x', 69: 'y', 70: 'z'}\n"
     ]
    }
   ],
   "source": [
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 2\n",
    "s_len = 40\n",
    "training_data = []\n",
    "next_char = []\n",
    "for i in range(0, len(text)-s_len, n_step):\n",
    "    string_to_add = text[i:i + s_len]\n",
    "    training_data.append(string_to_add)\n",
    "    if i + s_len < len(text)-1:\n",
    "        next_char.append(text[i + s_len])\n",
    "    \n",
    "# This is our end symbol $\n",
    "next_char.append(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48995\n",
      "48995\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))\n",
    "print(len(next_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.zeros((len(training_data), s_len, len(chars)))\n",
    "trainY = np.zeros((len(training_data), len(chars)))\n",
    "\n",
    "for sent_idx in range(0, len(training_data)):\n",
    "    curr_sentence = training_data[sent_idx]\n",
    "    curr_next_char = next_char[sent_idx]\n",
    "    for i, char in enumerate(curr_sentence):\n",
    "        trainX[sent_idx, i, char_to_index[char]] = 1\n",
    "    trainY[sent_idx, char_to_index[curr_next_char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48995, 40, 71)\n",
      "(48995, 71)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michellezhao/anaconda3/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "48995/48995 [==============================] - 31s 625us/step - loss: 2.9846\n",
      "Epoch 2/100\n",
      "48995/48995 [==============================] - 29s 587us/step - loss: 2.4464\n",
      "Epoch 3/100\n",
      "48995/48995 [==============================] - 29s 595us/step - loss: 2.2371\n",
      "Epoch 4/100\n",
      "48995/48995 [==============================] - 30s 608us/step - loss: 2.1288\n",
      "Epoch 5/100\n",
      "48995/48995 [==============================] - 30s 603us/step - loss: 2.0560\n",
      "Epoch 6/100\n",
      "48995/48995 [==============================] - 28s 578us/step - loss: 2.0028\n",
      "Epoch 7/100\n",
      "48995/48995 [==============================] - 29s 590us/step - loss: 1.9546\n",
      "Epoch 8/100\n",
      "48995/48995 [==============================] - 28s 572us/step - loss: 1.9126\n",
      "Epoch 9/100\n",
      "48995/48995 [==============================] - 29s 595us/step - loss: 1.8793\n",
      "Epoch 10/100\n",
      "48995/48995 [==============================] - 27s 559us/step - loss: 1.8504 - ETA: 0s - loss:\n",
      "Epoch 11/100\n",
      "48995/48995 [==============================] - 28s 578us/step - loss: 1.8214\n",
      "Epoch 12/100\n",
      "48995/48995 [==============================] - 27s 554us/step - loss: 1.7960\n",
      "Epoch 13/100\n",
      "48995/48995 [==============================] - 124s 3ms/step - loss: 1.7721\n",
      "Epoch 14/100\n",
      "48995/48995 [==============================] - 52s 1ms/step - loss: 1.7504\n",
      "Epoch 15/100\n",
      "48995/48995 [==============================] - 52s 1ms/step - loss: 1.7284\n",
      "Epoch 16/100\n",
      "48995/48995 [==============================] - 138s 3ms/step - loss: 1.7075\n",
      "Epoch 17/100\n",
      "48995/48995 [==============================] - 116s 2ms/step - loss: 1.6894\n",
      "Epoch 18/100\n",
      "48995/48995 [==============================] - 32s 649us/step - loss: 1.6704\n",
      "Epoch 19/100\n",
      "48995/48995 [==============================] - 34s 692us/step - loss: 1.6516\n",
      "Epoch 20/100\n",
      "48995/48995 [==============================] - 35s 722us/step - loss: 1.6341\n",
      "Epoch 21/100\n",
      "48995/48995 [==============================] - 32s 656us/step - loss: 1.6147\n",
      "Epoch 22/100\n",
      "48995/48995 [==============================] - 32s 658us/step - loss: 1.6012\n",
      "Epoch 23/100\n",
      "48995/48995 [==============================] - 29s 597us/step - loss: 1.5846\n",
      "Epoch 24/100\n",
      "48995/48995 [==============================] - 29s 599us/step - loss: 1.5661\n",
      "Epoch 25/100\n",
      "48995/48995 [==============================] - 29s 584us/step - loss: 1.5487\n",
      "Epoch 26/100\n",
      "48995/48995 [==============================] - 28s 575us/step - loss: 1.5300\n",
      "Epoch 27/100\n",
      "48995/48995 [==============================] - 30s 612us/step - loss: 1.5166\n",
      "Epoch 28/100\n",
      "48995/48995 [==============================] - 27s 547us/step - loss: 1.5015\n",
      "Epoch 29/100\n",
      "48995/48995 [==============================] - 27s 559us/step - loss: 1.4850\n",
      "Epoch 30/100\n",
      "48995/48995 [==============================] - 27s 546us/step - loss: 1.4667\n",
      "Epoch 31/100\n",
      "48995/48995 [==============================] - 26s 536us/step - loss: 1.44891\n",
      "Epoch 32/100\n",
      "48995/48995 [==============================] - 26s 529us/step - loss: 1.4314\n",
      "Epoch 33/100\n",
      "48995/48995 [==============================] - 26s 536us/step - loss: 1.4189\n",
      "Epoch 34/100\n",
      "48995/48995 [==============================] - 26s 540us/step - loss: 1.3985\n",
      "Epoch 35/100\n",
      "48995/48995 [==============================] - 27s 542us/step - loss: 1.3866\n",
      "Epoch 36/100\n",
      "48995/48995 [==============================] - 26s 533us/step - loss: 1.36882s - l\n",
      "Epoch 37/100\n",
      "48995/48995 [==============================] - 26s 532us/step - loss: 1.3579\n",
      "Epoch 38/100\n",
      "48995/48995 [==============================] - 26s 527us/step - loss: 1.3353\n",
      "Epoch 39/100\n",
      "48995/48995 [==============================] - 26s 531us/step - loss: 1.3208\n",
      "Epoch 40/100\n",
      "48995/48995 [==============================] - 26s 533us/step - loss: 1.3018\n",
      "Epoch 41/100\n",
      "48995/48995 [==============================] - 26s 534us/step - loss: 1.2876\n",
      "Epoch 42/100\n",
      "48995/48995 [==============================] - 26s 538us/step - loss: 1.2758\n",
      "Epoch 43/100\n",
      "48995/48995 [==============================] - 27s 549us/step - loss: 1.2562\n",
      "Epoch 44/100\n",
      "48995/48995 [==============================] - 26s 529us/step - loss: 1.2416\n",
      "Epoch 45/100\n",
      "48995/48995 [==============================] - 26s 529us/step - loss: 1.2259\n",
      "Epoch 46/100\n",
      "48995/48995 [==============================] - 26s 539us/step - loss: 1.2136\n",
      "Epoch 47/100\n",
      "48995/48995 [==============================] - 26s 540us/step - loss: 1.19780s - loss: 1 - ETA: 0s - loss: 1\n",
      "Epoch 48/100\n",
      "48995/48995 [==============================] - 28s 576us/step - loss: 1.1861\n",
      "Epoch 49/100\n",
      "48995/48995 [==============================] - 26s 539us/step - loss: 1.1671\n",
      "Epoch 50/100\n",
      "48995/48995 [==============================] - 26s 536us/step - loss: 1.1538\n",
      "Epoch 51/100\n",
      "48995/48995 [==============================] - 29s 596us/step - loss: 1.1444\n",
      "Epoch 52/100\n",
      "48995/48995 [==============================] - 27s 544us/step - loss: 1.1288\n",
      "Epoch 53/100\n",
      "48995/48995 [==============================] - 26s 533us/step - loss: 1.11530s - loss: 1.\n",
      "Epoch 54/100\n",
      "48995/48995 [==============================] - 27s 553us/step - loss: 1.1059\n",
      "Epoch 55/100\n",
      "48995/48995 [==============================] - 27s 555us/step - loss: 1.0910\n",
      "Epoch 56/100\n",
      "48995/48995 [==============================] - 27s 556us/step - loss: 1.0768\n",
      "Epoch 57/100\n",
      "48995/48995 [==============================] - 27s 545us/step - loss: 1.0737\n",
      "Epoch 58/100\n",
      "48995/48995 [==============================] - 27s 552us/step - loss: 1.0572\n",
      "Epoch 59/100\n",
      "48995/48995 [==============================] - 27s 549us/step - loss: 1.0418\n",
      "Epoch 60/100\n",
      "48995/48995 [==============================] - 29s 592us/step - loss: 1.0366\n",
      "Epoch 61/100\n",
      "48995/48995 [==============================] - 27s 553us/step - loss: 1.0259\n",
      "Epoch 62/100\n",
      "48995/48995 [==============================] - 27s 551us/step - loss: 1.0131\n",
      "Epoch 63/100\n",
      "48995/48995 [==============================] - 29s 585us/step - loss: 1.0084\n",
      "Epoch 64/100\n",
      "48995/48995 [==============================] - 29s 600us/step - loss: 0.9960\n",
      "Epoch 65/100\n",
      "48995/48995 [==============================] - 28s 576us/step - loss: 0.9961\n",
      "Epoch 66/100\n",
      "48995/48995 [==============================] - 28s 578us/step - loss: 0.9848\n",
      "Epoch 67/100\n",
      "48995/48995 [==============================] - 29s 582us/step - loss: 0.9736\n",
      "Epoch 68/100\n",
      "48995/48995 [==============================] - 29s 590us/step - loss: 0.9601\n",
      "Epoch 69/100\n",
      "48995/48995 [==============================] - 28s 577us/step - loss: 0.9564\n",
      "Epoch 70/100\n",
      "48995/48995 [==============================] - 28s 563us/step - loss: 0.9466\n",
      "Epoch 71/100\n",
      "48995/48995 [==============================] - 27s 554us/step - loss: 0.9441\n",
      "Epoch 72/100\n",
      "48995/48995 [==============================] - 27s 554us/step - loss: 0.9360\n",
      "Epoch 73/100\n",
      "48995/48995 [==============================] - 27s 546us/step - loss: 0.9314ETA: - ETA: 0s - loss: 0\n",
      "Epoch 74/100\n",
      "48995/48995 [==============================] - 26s 533us/step - loss: 0.9246\n",
      "Epoch 75/100\n",
      "48995/48995 [==============================] - 27s 549us/step - loss: 0.9175\n",
      "Epoch 76/100\n",
      "48995/48995 [==============================] - 28s 562us/step - loss: 0.9081\n",
      "Epoch 77/100\n",
      "48995/48995 [==============================] - 27s 552us/step - loss: 0.9043\n",
      "Epoch 78/100\n",
      "48995/48995 [==============================] - 28s 567us/step - loss: 0.8976\n",
      "Epoch 79/100\n",
      "48995/48995 [==============================] - ETA: 0s - loss: 0.889 - 30s 614us/step - loss: 0.8894\n",
      "Epoch 80/100\n",
      "48995/48995 [==============================] - 27s 544us/step - loss: 0.8871\n",
      "Epoch 81/100\n",
      "48995/48995 [==============================] - 28s 569us/step - loss: 0.8824\n",
      "Epoch 82/100\n",
      "48995/48995 [==============================] - 28s 563us/step - loss: 0.8773\n",
      "Epoch 83/100\n",
      "48995/48995 [==============================] - 26s 533us/step - loss: 0.8718\n",
      "Epoch 84/100\n",
      "48995/48995 [==============================] - 28s 573us/step - loss: 0.8670\n",
      "Epoch 85/100\n",
      "48995/48995 [==============================] - 27s 553us/step - loss: 0.8642\n",
      "Epoch 86/100\n",
      "48995/48995 [==============================] - 27s 550us/step - loss: 0.8603\n",
      "Epoch 87/100\n",
      "48995/48995 [==============================] - 28s 572us/step - loss: 0.8537\n",
      "Epoch 88/100\n",
      "48995/48995 [==============================] - 27s 560us/step - loss: 0.85180s - loss\n",
      "Epoch 89/100\n",
      "48995/48995 [==============================] - 27s 548us/step - loss: 0.8404TA: 0s - loss: 0.\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48995/48995 [==============================] - 28s 564us/step - loss: 0.8449\n",
      "Epoch 91/100\n",
      "48995/48995 [==============================] - 27s 551us/step - loss: 0.8386\n",
      "Epoch 92/100\n",
      "48995/48995 [==============================] - 27s 557us/step - loss: 0.8352\n",
      "Epoch 93/100\n",
      "48995/48995 [==============================] - 28s 574us/step - loss: 0.8217\n",
      "Epoch 94/100\n",
      "48995/48995 [==============================] - 29s 583us/step - loss: 0.8271\n",
      "Epoch 95/100\n",
      "48995/48995 [==============================] - 28s 570us/step - loss: 0.8180\n",
      "Epoch 96/100\n",
      "48995/48995 [==============================] - 28s 561us/step - loss: 0.8198\n",
      "Epoch 97/100\n",
      "48995/48995 [==============================] - 27s 548us/step - loss: 0.8149\n",
      "Epoch 98/100\n",
      "48995/48995 [==============================] - 29s 588us/step - loss: 0.8088\n",
      "Epoch 99/100\n",
      "48995/48995 [==============================] - 28s 569us/step - loss: 0.8084\n",
      "Epoch 100/100\n",
      "48995/48995 [==============================] - 29s 590us/step - loss: 0.8061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1835571e48>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(s_len, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#train the model\n",
    "\n",
    "#print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop')\n",
    "model.fit(trainX, trainY, batch_size = 128, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "Than should his bearth of love and thing disheded.\n",
      "That heart of lect born with thy shall stear,\n",
      "And to be cores me an me, and buty,\n",
      "And such than should have fair of the love,\n",
      "That this for to eare that with thy reserv,\n",
      "And to nour livess wham I to be others,\n",
      "And my to be buther bust which I doobs,\n",
      "  And surmer best for my self sime, in sean,\n",
      "And it himpith and did comprits of all\n",
      "otherst,\n",
      "And thin me branget me faill be than she dothee,\n",
      "  Thou art his black and say the sum for I shade,\n",
      "The eremy with belund is in the fire,\n",
      "The base the reston shall batien to give his drade.\n",
      "The hath in my sourngions on thy gract,\n",
      "And to have to me besing and is thy sied,\n",
      "The sean and thie still that thou shave, the owe,\n",
      "  Thes rumbers no mann, will thou mint ewe,\n",
      "  if to the surmpresse than she will grace,\n",
      "Withingt thou thy sweet fare's now thou art,\n",
      "Where in the shall wat my most mind eyes hear,\n",
      "Whon thy shall be that foul such mise shell,\n",
      "Whounst my self all thans and thise sooll\n",
      "t and seem,\n",
      "And th\n"
     ]
    }
   ],
   "source": [
    "start_index = np.random.randint(0, len(text)-s_len-1)\n",
    "start_index = 50\n",
    "sentence = text[start_index: start_index + s_len]\n",
    "sentence = \"shall i compare thee to a summer's day?\\n\"\n",
    "sequence = sentence\n",
    "for i in range(1000):\n",
    "    x_pred = np.zeros((1, s_len, len(chars)))\n",
    "    for j, char in enumerate(sentence):\n",
    "        x_pred[0, j, char_to_index[char]] = 1.\n",
    "        \n",
    "    predictions = np.array(model.predict(x_pred)[0])\n",
    "    max_index = np.argmax(predictions)\n",
    "    next_char = index_to_char[max_index]\n",
    "    sentence = sentence[1:] + next_char\n",
    "    sequence += next_char\n",
    "\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "index = np.random.choice(len(chars), 1, predictions.all())\n",
    "print(index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ire increase,\n",
      "That thereby beauty's rose see flow Ind sweart\n",
      "That of you sway woth from wey shy bideds,\n",
      "And mire hele reain be rained in you.\n",
      "\n",
      "\n",
      "                    12ed I camputain inding me ,\n",
      "Teme in the weery wertting selfour sight\n",
      "robt comber yot curnton to toke you sheart,\n",
      "Whorlf at atbeasts and for mind ewe dig:ue,\n",
      "atissred out doth ver,\n",
      "at enit monks the rooner to the owers make,\n",
      "Whoreming thee 'it stmer, frulless in thy hings,\n",
      "And be2ty creets shame arver in the is detheed.\n",
      "Thou a not thich my worch you bid, bul dead.\n",
      "Then the eanth thou dongubhicg ofor mode,\n",
      "And hate sungers of thy night,\n",
      "How me, no or the vine hempastis wakl mus,\n",
      "With uple miny of bain ampelsiof how rasth.\n",
      "That pursh the thou my lies, why fordity nombull,\n",
      "And loat of lais and not for shind soo\n",
      ",\n",
      "So waln I ar form ar monking heave.\n",
      "When I an umenor bely faily done, Inong.\n",
      "vend ny llone, without mondon merlinges ampare ince sweirnge,\n",
      "When I ar mventer, and then me nom thou art.\n",
      "  Ther that in my singll to mise me,\n",
      "A dough withiven though druphy oftren\n"
     ]
    }
   ],
   "source": [
    "start_index = np.random.randint(0, len(text)-s_len-1)\n",
    "start_index = 50\n",
    "sentence = text[start_index: start_index + s_len]\n",
    "sequence = sentence\n",
    "for i in range(1000):\n",
    "    x_pred = np.zeros((1, s_len, len(chars)))\n",
    "    for j, char in enumerate(sentence):\n",
    "        x_pred[0, j, char_to_index[char]] = 1.\n",
    "        \n",
    "    predictions = np.array(model.predict(x_pred)[0])\n",
    "    #max_index = np.argmax(predictions)\n",
    "    index = np.random.choice(len(chars), 1, p=predictions)[0]\n",
    "    next_char = index_to_char[index]\n",
    "    sentence = sentence[1:] + next_char\n",
    "    sequence += next_char\n",
    "\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "71\n",
      "0.730325\n",
      "1\n",
      "[  6.94062852e-04   7.30325460e-01   1.18445519e-06   7.78295682e-04\n",
      "   1.11060372e-05   7.64163997e-06   2.44515250e-03   5.60624612e-05\n",
      "   2.19699068e-04   5.26500116e-06   2.41895978e-05   3.95123561e-06\n",
      "   5.21234870e-06   6.64439176e-06   5.94937501e-06   1.87032833e-06\n",
      "   3.34175570e-06   3.50042365e-06   3.44382511e-06   8.43042362e-05\n",
      "   1.56733108e-06   3.46993038e-05   2.34842213e-04   9.26001521e-05\n",
      "   1.01433932e-06   5.02007242e-06   4.80299718e-07   6.68773864e-05\n",
      "   6.11881489e-07   2.46848376e-05   1.70587271e-03   2.24313212e-09\n",
      "   1.10972440e-08   1.80436655e-05   3.18466082e-05   2.87337753e-05\n",
      "   6.46860281e-05   4.98809823e-06   9.20415971e-07   8.35577594e-05\n",
      "   7.51566084e-04   7.88684019e-07   1.68324221e-09   1.43170066e-04\n",
      "   4.98383315e-06   1.99320279e-02   3.33607686e-03   2.89679645e-03\n",
      "   1.38006210e-02   2.89650504e-02   6.97758840e-03   1.96235511e-03\n",
      "   1.81529578e-02   1.79779790e-02   2.02268620e-05   7.90904975e-04\n",
      "   1.11462157e-02   7.73747545e-03   2.35713702e-02   2.51614377e-02\n",
      "   2.19918229e-03   1.84778310e-05   1.21617801e-02   1.51645746e-02\n",
      "   3.21238004e-02   4.12663165e-03   9.03391687e-04   8.15809052e-03\n",
      "   1.30535655e-05   4.71350085e-03   5.96043492e-07]\n"
     ]
    }
   ],
   "source": [
    "print(len(x_pred))\n",
    "print(len(predictions))\n",
    "print(max(predictions))\n",
    "print(np.argmax(predictions))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
