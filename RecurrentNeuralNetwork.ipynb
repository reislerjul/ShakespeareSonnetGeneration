{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function from set 6 to parse the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_observations(text):\n",
    "    # Convert text to dataset.\n",
    "    lines = [line.split() for line in text.split('\\n') if line.split()]\n",
    "\n",
    "    obs_counter = 0\n",
    "    obs = []\n",
    "    obs_map = {}\n",
    "\n",
    "    for line in lines:\n",
    "        obs_elem = []\n",
    "        \n",
    "        for word in line:\n",
    "            word = re.sub(r'[^\\w]', '', word).lower()\n",
    "            if word not in obs_map:\n",
    "                # Add unique words to the observations map.\n",
    "                obs_map[word] = obs_counter\n",
    "                obs_counter += 1\n",
    "            \n",
    "            # Add the encoded word.\n",
    "            obs_elem.append(obs_map[word])\n",
    "        \n",
    "        # Add the encoded sequence.\n",
    "        obs.append(obs_elem)\n",
    "\n",
    "    return obs, obs_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = open(os.path.join(os.getcwd(), \\\n",
    "                         'Release/data/shakespeare.txt')).read()\n",
    "obs, obs_map = parse_observations(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chars = list(text)\n",
    "chars = sorted(list(set(text)))\n",
    "data_size, vocab_size = len(text), len(chars)\n",
    "char_to_index = {char:idx for idx, char in enumerate(chars)}\n",
    "index_to_char = {idx:char for idx, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_step = 2\n",
    "s_len = 40\n",
    "training_data = []\n",
    "next_char = []\n",
    "for i in range(0, len(text)-s_len, n_step):\n",
    "    string_to_add = text[i:i + s_len]\n",
    "    training_data.append(string_to_add)\n",
    "    if i + s_len < len(text)-1:\n",
    "        next_char.append(text[i + s_len])\n",
    "    \n",
    "# This is our end symbol: a space\n",
    "next_char.append(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX = np.zeros((len(training_data), s_len, len(chars)))\n",
    "trainY = np.zeros((len(training_data), len(chars)))\n",
    "\n",
    "for sent_idx in range(0, len(training_data)):\n",
    "    curr_sentence = training_data[sent_idx]\n",
    "    curr_next_char = next_char[sent_idx]\n",
    "    for i, char in enumerate(curr_sentence):\n",
    "        trainX[sent_idx, i, char_to_index[char]] = 1\n",
    "    trainY[sent_idx, char_to_index[curr_next_char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_poem_maxprob(length, random = False):\n",
    "    if random == True:\n",
    "        start_index = np.random.randint(0, len(text)-s_len-1)\n",
    "        start_index = 50\n",
    "        sentence = text[start_index: start_index + s_len]\n",
    "    else:\n",
    "        sentence = \"shall i compare thee to a summer's day?\\n\"\n",
    "    sequence = sentence\n",
    "    for i in range(length):\n",
    "        x_pred = np.zeros((1, s_len, len(chars)))\n",
    "        for j, char in enumerate(sentence):\n",
    "            x_pred[0, j, char_to_index[char]] = 1.\n",
    "\n",
    "        predictions = np.array(model.predict(x_pred)[0])\n",
    "        max_index = np.argmax(predictions)\n",
    "        next_char = index_to_char[max_index]\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sequence += next_char\n",
    "\n",
    "    print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_poem_randprob(length, random = False):\n",
    "    if random == True:\n",
    "        start_index = np.random.randint(0, len(text)-s_len-1)\n",
    "        start_index = 50\n",
    "        sentence = text[start_index: start_index + s_len]\n",
    "    else:\n",
    "        sentence = \"shall i compare thee to a summer's day?\\n\"\n",
    "    sequence = sentence\n",
    "    for i in range(length):\n",
    "        x_pred = np.zeros((1, s_len, len(chars)))\n",
    "        for j, char in enumerate(sentence):\n",
    "            x_pred[0, j, char_to_index[char]] = 1.\n",
    "\n",
    "        predictions = np.array(model.predict(x_pred)[0])\n",
    "        #max_index = np.argmax(predictions)\n",
    "        index = np.random.choice(len(chars), 1, p=predictions)[0]\n",
    "        next_char = index_to_char[index]\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sequence += next_char\n",
    "\n",
    "    print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we use an LSTM of 150 layers, with no dropout. It has a standard fully-connected output layer with a softmax activation. We train it for 100 epochs. We print out the poems generated by selecting the maximum probability letter and also the poems generated by selecting a random character weighted based on the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michellezhao/anaconda3/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "bach trach the breathess grass and sur of yourn, heartnes, lees.\n",
      "Fould give I lavel weech ours a fair trie,\n",
      "Love sunkeriew, her thought that love of thy preased,\n",
      "That have parser cimed and for when lives,\n",
      "And make sumbless with so steelldst trim such frow hided be,\n",
      "  Tile minden the sacke where not mellengs crasses firit,\n",
      "My ming no black where eyes, her best brainy?\n",
      "That Love of chankest where thou all me arthint,\n",
      "fore tho  ives thy swornt balds and do clear,\n",
      "Sa all the paint a dabiest in my pantier.\n",
      "\n",
      "\n",
      "                   112\n",
      ", alt my that thou mayst thou dost crnelds,\n",
      "  Is of their vir uppeef, or erst pownors,\n",
      "And in a beact as is makes sommmanfoume,\n",
      "And that hath my truectsing for a wrons,\n",
      "For thee if heaven's sors, and I long thes,\n",
      "Is in the creace, beingent of lige,\n",
      "And seaver cove with in he pay one receive,\n",
      "The earth on my nig beadt and this spring,\n",
      "When I a pooreare though thy shard,\n",
      "Thy segs and voors excuming both dosMy hire,\n",
      "When somblesso soons, then thou dies heart,\n",
      "Where c\n",
      "shall i compare thee to a summer's day?\n",
      "For all be kired faires thy shouldst leas.\n",
      "\n",
      "\n",
      "                   11o :fact that you thil becaminion of suched,\n",
      "O chase dith in the parts a desif in nou,\n",
      "Nor no, I wor man'st tonguens, arose his to,\n",
      "gind hore with than thos sweet belassicape jectice,\n",
      "Beir such me flies tarked no jucgston their you,\n",
      "They my show felf on From furthy inved,\n",
      "And guect that sweat cansed hade this thy poart,\n",
      "on all then seems in intainge wheretres,\n",
      "And eyes treas, now bandest nighast was est,\n",
      "For are by and felings with that fairs,\n",
      "knided have sweets fair waily thou art thoughtrer thee:\n",
      "Stall's mare's remong head with sulf erone,\n",
      "But stranging in a vanguelt losks beart.\n",
      "where bask with pripition pprifient leave,\n",
      "Are but as my food, that I south,\n",
      " penting me tile's fingless cleseed says\n",
      "With seasll giety says, as thou art astered car,\n",
      "bast lodgarons fiend and sen dorlart.\n",
      "That thou and lought thee shall gonded a easered leas:\n",
      "  bure my novering speny, gear me wites,\n",
      "And seaked entreat be thou did, whone is glon\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape=(s_len, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#train the model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop')\n",
    "model.fit(trainX, trainY, batch_size = 128, nb_epoch = 100, verbose = 0)\n",
    "print_poem_maxprob(1000)\n",
    "print_poem_randprob(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we use an LSTM of 100 layers, with a dropout of 0.2. It has a standard fully-connected output layer with a softmax activation. We train it for 150 epochs. We print out the poems generated by selecting the maximum probability letter and also the poems generated by selecting a random character weighted based on the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michellezhao/anaconda3/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "So be mased, now for me of my love notg,\n",
      "Which you where where thou artures my sear,\n",
      "And to my self tho  blassing he moull dest,\n",
      "That the bads a dost thou when than you hishsestrend the hearte,\n",
      "  Thes fay to the thee that dear the beauty,\n",
      "That to the where of all my day,\n",
      "  Thes fair beauty mose,\n",
      "    h the wiret my user's withor what be forg,\n",
      "llance I to meturn ortance of he dose face,\n",
      "Which not love's with gureforen in pray,\n",
      "  Then sham thou beauty, and the ull the stand,\n",
      "That the should of thee, when thou with the witledstred,\n",
      "And in the braintor me hat hamd to steed,\n",
      "That to the beauty world do I of gelfay,\n",
      "When thy self and fall than thee beauty's bear,\n",
      "And woth thy selfor me, ho  art the sond,\n",
      "That the self whe reasuned and hear with thee,\n",
      "Which thee I bast thou should that mene, Are then stringle doth lightlss.\n",
      "I soon so fail theee speak that thou moun.\n",
      "\n",
      "\n",
      "                   112\n",
      "O for the wantes shath my seat with the have nowe thos slowe shall gray,\n",
      "That har the more what hath of \n",
      "shall i compare thee to a summer's day?\n",
      "  And hears for mar's grows whice  it in wored,\n",
      "Wholl of thy defale not and yot beak,\n",
      "Stail of thy seaces is made comine ever be.\n",
      "  Ther with ligin stoul, as inver in proun,\n",
      "When in she ristew, the conty desigh pond,\n",
      "Thein in thee to des cirpoust of me,\n",
      "Which you, fall the time in mose and conteed,\n",
      "Then to the loodil, with his gromess and gay,\n",
      "That ste might that wouth tougr consll senl,\n",
      "  in my leastow enou aid in the but of hast,\n",
      ":luts of seen, when blamgs Inle hamuse siFor seal's grown,\n",
      "  I witt you mine belaingis detis senc, well:\n",
      "  But th thou art wear thou abt me is bend,\n",
      "Fir thee, whereank of to me, thee are is glow.\n",
      "  To thal my seaw, all make gongnestich doth string,\n",
      "Love's with me, whasings oun boundsy of drays,\n",
      "As eniteet shangling a bort,\n",
      "  Lis fow lich whath for love do blow of thee,\n",
      "  Then they thy foul ming own ret me sincelss.\n",
      "o brantt thou lovn's weth onghantest theost\n",
      "If htame upon that see in poot crace,\n",
      "Whinst in a wore, where it antients And hee,\n",
      "That be offatreed \n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(s_len, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#train the model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop')\n",
    "model.fit(trainX, trainY, batch_size = 128, nb_epoch = 150, verbose = 0)\n",
    "print_poem_maxprob(1000)\n",
    "print_poem_randprob(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, we use the Adam optimizer with a LSTM of size 200, run for 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michellezhao/anaconda3/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "Shence a love to me for thould may love the stren,\n",
      "  For will great from thy strent form on pear,\n",
      "Which reake my nee of thy his my and preasure,\n",
      "But thy sins for the self will be to steet,\n",
      "Which my beauty the fire or summers bad,\n",
      "My beauty do the beauty shall of to spent,\n",
      "Shall beauty's ragurnoth bothers that then seew,\n",
      "Betith that false of thy recorned doth re.\n",
      "  There wo love thee that I do beatt, when thou art owe to me,\n",
      "  Tind they doin of thy five my love sweet forghad,\n",
      "And suin the erave ancemmint,\n",
      "  As chose the will with heart, and thou art owned,\n",
      "I aut thou makes if thy beauty day,\n",
      "  To thus forthing in their day, and than the will whe holl by menders,\n",
      "That the rome there would baingters of you,\n",
      "When thou art fair this thy self which like to sheat,\n",
      "And such and therse paitter what thou art,\n",
      "If tells no love though showell be the sure,\n",
      "And sun of more belt and math the grower,\n",
      "Which in the farse my self will beauty,\n",
      "And then in the world but fal thise did,\n",
      "Is that thou the subs\n",
      "shall i compare thee to a summer's day?\n",
      "Shey it this fair that hear shall be it sund?\n",
      "Elen that nither and, and thing and swall,\n",
      "Whangis thou and unquianting shall those.\n",
      "  Aor love the easthron wreck it south's amane,\n",
      "And thy pewaives raziknoun bain thy breath,\n",
      "And heart'm lives in a mertain eyou,\n",
      "With thee, threpcasisy madl cheld all when I shol.\n",
      "\n",
      "\n",
      "                   96\n",
      "But the longun my corfelling and arake,\n",
      "I comeer bented time thy self arathing,\n",
      "To felshis balter bait thou showins will,\n",
      "Lenge gilt your that when I precelfed's reing.\n",
      "And all by the self do, charte my lead.\n",
      "Yot I note to me, ond it not farte, congeing.\n",
      "Po sook in this, and that the stoul styee:\n",
      "A bl thy light world's sweets oud a selfore,\n",
      "  You do the know, which I but-'limy swied,\n",
      "What saments abter fallth in wered was,\n",
      "Upon thy evee thy will not so aly sich,\n",
      "I hat be arons bus y thee sinmer our:\n",
      "Mare buther be the paries the verfey themes:\n",
      "Is fense think show whe ealley fallst,\n",
      "And in the cadnam shole-s, when I press ine,\n",
      "Whight of thy subjoss of soul d\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(s_len, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#train the model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam')\n",
    "model.fit(trainX, trainY, batch_size = 128, nb_epoch = 50, verbose = 0)\n",
    "print_poem_maxprob(1000)\n",
    "print_poem_randprob(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
