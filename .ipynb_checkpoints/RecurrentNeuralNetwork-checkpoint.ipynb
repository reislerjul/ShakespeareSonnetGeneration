{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function from set 6 to parse the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_observations(text):\n",
    "    # Convert text to dataset.\n",
    "    lines = [line.split() for line in text.split('\\n') if line.split()]\n",
    "\n",
    "    obs_counter = 0\n",
    "    obs = []\n",
    "    obs_map = {}\n",
    "\n",
    "    for line in lines:\n",
    "        obs_elem = []\n",
    "        \n",
    "        for word in line:\n",
    "            word = re.sub(r'[^\\w]', '', word).lower()\n",
    "            if word not in obs_map:\n",
    "                # Add unique words to the observations map.\n",
    "                obs_map[word] = obs_counter\n",
    "                obs_counter += 1\n",
    "            \n",
    "            # Add the encoded word.\n",
    "            obs_elem.append(obs_map[word])\n",
    "        \n",
    "        # Add the encoded sequence.\n",
    "        obs.append(obs_elem)\n",
    "\n",
    "    return obs, obs_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = open(os.path.join(os.getcwd(), \\\n",
    "                         'Release/data/shakespeare.txt')).read()\n",
    "obs, obs_map = parse_observations(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we’re doing a character-based LSTM, first, we create a sorted dictionary of all characters used in the entire text, where each character is assigned an index. We create a two dictionaries, a character-to-index dictionary and an index-to-character dictionary, which will be used later for easy access to characters and indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chars = list(text)\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {char:idx for idx, char in enumerate(chars)}\n",
    "index_to_char = {idx:char for idx, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We break up the text into 40-character-length sequences, which will be our training X’s. For each 40 character sequence, the corresponding training Y will be the next character that follows. These input pairs will be the data that we train the LSTM on. In our code, we can also specify a step size (n_step) so that we can take the 40-character sequences at different intervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_step = 2\n",
    "s_len = 40\n",
    "training_data = []\n",
    "next_char = []\n",
    "for i in range(0, len(text)-s_len, n_step):\n",
    "    string_to_add = text[i:i + s_len]\n",
    "    training_data.append(string_to_add)\n",
    "    if i + s_len < len(text)-1:\n",
    "        next_char.append(text[i + s_len])\n",
    "    \n",
    "# This is our end symbol: a space\n",
    "next_char.append(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras LSTM takes a 3-dimensional input for X and a 2-dimensional input for Y. The dimensions of the X input are [number of sequences x sequence length(40) x length of character vector]. The dimensions of Y are [number of sequences x length of character vector]. The X-input is the list of 40-character sequences, where each position contains the vectorized character. The Y-input is the list of characters following the 40-char sequences, where each character is also vectorized. We manually 1-hot vectorize the characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX = np.zeros((len(training_data), s_len, len(chars)))\n",
    "trainY = np.zeros((len(training_data), len(chars)))\n",
    "\n",
    "for sent_idx in range(0, len(training_data)):\n",
    "    curr_sentence = training_data[sent_idx]\n",
    "    curr_next_char = next_char[sent_idx]\n",
    "    for i, char in enumerate(curr_sentence):\n",
    "        trainX[sent_idx, i, char_to_index[char]] = 1\n",
    "    trainY[sent_idx, char_to_index[curr_next_char]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for printing out poems from this LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_poem_maxprob(length, random = False):\n",
    "    if random == True:\n",
    "        start_index = np.random.randint(0, len(text)-s_len-1)\n",
    "        start_index = 50\n",
    "        sentence = text[start_index: start_index + s_len]\n",
    "    else:\n",
    "        sentence = \"shall i compare thee to a summer's day?\\n\"\n",
    "    sequence = sentence\n",
    "    for i in range(length):\n",
    "        x_pred = np.zeros((1, s_len, len(chars)))\n",
    "        for j, char in enumerate(sentence):\n",
    "            x_pred[0, j, char_to_index[char]] = 1.\n",
    "\n",
    "        predictions = np.array(model.predict(x_pred)[0])\n",
    "        max_index = np.argmax(predictions)\n",
    "        next_char = index_to_char[max_index]\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sequence += next_char\n",
    "\n",
    "    print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_poem_randprob(length, random = False):\n",
    "    if random == True:\n",
    "        start_index = np.random.randint(0, len(text)-s_len-1)\n",
    "        start_index = 50\n",
    "        sentence = text[start_index: start_index + s_len]\n",
    "    else:\n",
    "        sentence = \"shall i compare thee to a summer's day?\\n\"\n",
    "    sequence = sentence\n",
    "    for i in range(length):\n",
    "        x_pred = np.zeros((1, s_len, len(chars)))\n",
    "        for j, char in enumerate(sentence):\n",
    "            x_pred[0, j, char_to_index[char]] = 1.\n",
    "\n",
    "        predictions = np.array(model.predict(x_pred)[0])\n",
    "        #max_index = np.argmax(predictions)\n",
    "        index = np.random.choice(len(chars), 1, p=predictions)[0]\n",
    "        next_char = index_to_char[index]\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sequence += next_char\n",
    "\n",
    "    print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we use an LSTM layer of 150 units, with no dropout. It has a standard fully-connected output layer with a softmax activation. We train it for 100 epochs. We print out the poems generated by selecting the maximum probability letter and also the poems generated by selecting a random character weighted based on the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michellezhao/anaconda3/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "bach trach the breathess grass and sur of yourn, heartnes, lees.\n",
      "Fould give I lavel weech ours a fair trie,\n",
      "Love sunkeriew, her thought that love of thy preased,\n",
      "That have parser cimed and for when lives,\n",
      "And make sumbless with so steelldst trim such frow hided be,\n",
      "  Tile minden the sacke where not mellengs crasses firit,\n",
      "My ming no black where eyes, her best brainy?\n",
      "That Love of chankest where thou all me arthint,\n",
      "fore tho  ives thy swornt balds and do clear,\n",
      "Sa all the paint a dabiest in my pantier.\n",
      "\n",
      "\n",
      "                   112\n",
      ", alt my that thou mayst thou dost crnelds,\n",
      "  Is of their vir uppeef, or erst pownors,\n",
      "And in a beact as is makes sommmanfoume,\n",
      "And that hath my truectsing for a wrons,\n",
      "For thee if heaven's sors, and I long thes,\n",
      "Is in the creace, beingent of lige,\n",
      "And seaver cove with in he pay one receive,\n",
      "The earth on my nig beadt and this spring,\n",
      "When I a pooreare though thy shard,\n",
      "Thy segs and voors excuming both dosMy hire,\n",
      "When somblesso soons, then thou dies heart,\n",
      "Where c\n",
      "shall i compare thee to a summer's day?\n",
      "For all be kired faires thy shouldst leas.\n",
      "\n",
      "\n",
      "                   11o :fact that you thil becaminion of suched,\n",
      "O chase dith in the parts a desif in nou,\n",
      "Nor no, I wor man'st tonguens, arose his to,\n",
      "gind hore with than thos sweet belassicape jectice,\n",
      "Beir such me flies tarked no jucgston their you,\n",
      "They my show felf on From furthy inved,\n",
      "And guect that sweat cansed hade this thy poart,\n",
      "on all then seems in intainge wheretres,\n",
      "And eyes treas, now bandest nighast was est,\n",
      "For are by and felings with that fairs,\n",
      "knided have sweets fair waily thou art thoughtrer thee:\n",
      "Stall's mare's remong head with sulf erone,\n",
      "But stranging in a vanguelt losks beart.\n",
      "where bask with pripition pprifient leave,\n",
      "Are but as my food, that I south,\n",
      " penting me tile's fingless cleseed says\n",
      "With seasll giety says, as thou art astered car,\n",
      "bast lodgarons fiend and sen dorlart.\n",
      "That thou and lought thee shall gonded a easered leas:\n",
      "  bure my novering speny, gear me wites,\n",
      "And seaked entreat be thou did, whone is glon\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape=(s_len, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#train the model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop')\n",
    "model.fit(trainX, trainY, batch_size = 128, nb_epoch = 100, verbose = 0)\n",
    "print_poem_maxprob(1000)\n",
    "print_poem_randprob(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we use an LSTM layer of 100 units, with a dropout of 0.2. It has a standard fully-connected output layer with a softmax activation. We train it for 150 epochs. We print out the poems generated by selecting the maximum probability letter and also the poems generated by selecting a random character weighted based on the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michellezhao/anaconda3/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "So be mased, now for me of my love notg,\n",
      "Which you where where thou artures my sear,\n",
      "And to my self tho  blassing he moull dest,\n",
      "That the bads a dost thou when than you hishsestrend the hearte,\n",
      "  Thes fay to the thee that dear the beauty,\n",
      "That to the where of all my day,\n",
      "  Thes fair beauty mose,\n",
      "    h the wiret my user's withor what be forg,\n",
      "llance I to meturn ortance of he dose face,\n",
      "Which not love's with gureforen in pray,\n",
      "  Then sham thou beauty, and the ull the stand,\n",
      "That the should of thee, when thou with the witledstred,\n",
      "And in the braintor me hat hamd to steed,\n",
      "That to the beauty world do I of gelfay,\n",
      "When thy self and fall than thee beauty's bear,\n",
      "And woth thy selfor me, ho  art the sond,\n",
      "That the self whe reasuned and hear with thee,\n",
      "Which thee I bast thou should that mene, Are then stringle doth lightlss.\n",
      "I soon so fail theee speak that thou moun.\n",
      "\n",
      "\n",
      "                   112\n",
      "O for the wantes shath my seat with the have nowe thos slowe shall gray,\n",
      "That har the more what hath of \n",
      "shall i compare thee to a summer's day?\n",
      "  And hears for mar's grows whice  it in wored,\n",
      "Wholl of thy defale not and yot beak,\n",
      "Stail of thy seaces is made comine ever be.\n",
      "  Ther with ligin stoul, as inver in proun,\n",
      "When in she ristew, the conty desigh pond,\n",
      "Thein in thee to des cirpoust of me,\n",
      "Which you, fall the time in mose and conteed,\n",
      "Then to the loodil, with his gromess and gay,\n",
      "That ste might that wouth tougr consll senl,\n",
      "  in my leastow enou aid in the but of hast,\n",
      ":luts of seen, when blamgs Inle hamuse siFor seal's grown,\n",
      "  I witt you mine belaingis detis senc, well:\n",
      "  But th thou art wear thou abt me is bend,\n",
      "Fir thee, whereank of to me, thee are is glow.\n",
      "  To thal my seaw, all make gongnestich doth string,\n",
      "Love's with me, whasings oun boundsy of drays,\n",
      "As eniteet shangling a bort,\n",
      "  Lis fow lich whath for love do blow of thee,\n",
      "  Then they thy foul ming own ret me sincelss.\n",
      "o brantt thou lovn's weth onghantest theost\n",
      "If htame upon that see in poot crace,\n",
      "Whinst in a wore, where it antients And hee,\n",
      "That be offatreed \n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(s_len, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#train the model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop')\n",
    "model.fit(trainX, trainY, batch_size = 128, nb_epoch = 150, verbose = 0)\n",
    "print_poem_maxprob(1000)\n",
    "print_poem_randprob(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, we tested an LSTM layer of 200 units, with a dropout of 0.2. It has a standard fully-connected output layer with a softmax activation. We train it for 50 epochs using the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michellezhao/anaconda3/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "Shence a love to me for thould may love the stren,\n",
      "  For will great from thy strent form on pear,\n",
      "Which reake my nee of thy his my and preasure,\n",
      "But thy sins for the self will be to steet,\n",
      "Which my beauty the fire or summers bad,\n",
      "My beauty do the beauty shall of to spent,\n",
      "Shall beauty's ragurnoth bothers that then seew,\n",
      "Betith that false of thy recorned doth re.\n",
      "  There wo love thee that I do beatt, when thou art owe to me,\n",
      "  Tind they doin of thy five my love sweet forghad,\n",
      "And suin the erave ancemmint,\n",
      "  As chose the will with heart, and thou art owned,\n",
      "I aut thou makes if thy beauty day,\n",
      "  To thus forthing in their day, and than the will whe holl by menders,\n",
      "That the rome there would baingters of you,\n",
      "When thou art fair this thy self which like to sheat,\n",
      "And such and therse paitter what thou art,\n",
      "If tells no love though showell be the sure,\n",
      "And sun of more belt and math the grower,\n",
      "Which in the farse my self will beauty,\n",
      "And then in the world but fal thise did,\n",
      "Is that thou the subs\n",
      "shall i compare thee to a summer's day?\n",
      "Shey it this fair that hear shall be it sund?\n",
      "Elen that nither and, and thing and swall,\n",
      "Whangis thou and unquianting shall those.\n",
      "  Aor love the easthron wreck it south's amane,\n",
      "And thy pewaives raziknoun bain thy breath,\n",
      "And heart'm lives in a mertain eyou,\n",
      "With thee, threpcasisy madl cheld all when I shol.\n",
      "\n",
      "\n",
      "                   96\n",
      "But the longun my corfelling and arake,\n",
      "I comeer bented time thy self arathing,\n",
      "To felshis balter bait thou showins will,\n",
      "Lenge gilt your that when I precelfed's reing.\n",
      "And all by the self do, charte my lead.\n",
      "Yot I note to me, ond it not farte, congeing.\n",
      "Po sook in this, and that the stoul styee:\n",
      "A bl thy light world's sweets oud a selfore,\n",
      "  You do the know, which I but-'limy swied,\n",
      "What saments abter fallth in wered was,\n",
      "Upon thy evee thy will not so aly sich,\n",
      "I hat be arons bus y thee sinmer our:\n",
      "Mare buther be the paries the verfey themes:\n",
      "Is fense think show whe ealley fallst,\n",
      "And in the cadnam shole-s, when I press ine,\n",
      "Whight of thy subjoss of soul d\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(s_len, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#train the model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam')\n",
    "model.fit(trainX, trainY, batch_size = 128, nb_epoch = 50, verbose = 0)\n",
    "print_poem_maxprob(1000)\n",
    "print_poem_randprob(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Analysis of Variation of Temperatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_poem_temp(length, temp, random = False):\n",
    "    if random == True:\n",
    "        start_index = np.random.randint(0, len(text)-s_len-1)\n",
    "        start_index = 50\n",
    "        sentence = text[start_index: start_index + s_len]\n",
    "    else:\n",
    "        sentence = \"shall i compare thee to a summer's day?\\n\"\n",
    "    sequence = sentence\n",
    "    for i in range(length):\n",
    "        x_pred = np.zeros((1, s_len, len(chars)))\n",
    "        for j, char in enumerate(sentence):\n",
    "            x_pred[0, j, char_to_index[char]] = 1.\n",
    "\n",
    "        predictions = np.array(model.predict(x_pred)[0])\n",
    "        for p in predictions:\n",
    "            p = np.exp(p/temp)\n",
    "        div = sum(predictions)\n",
    "        for p in predictions:\n",
    "            p = p/np.sum(predictions)\n",
    "        #predictions = preds\n",
    "        #max_index = np.argmax(predictions)\n",
    "        index = np.random.choice(len(chars), 1, p=predictions)[0]\n",
    "        next_char = index_to_char[index]\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sequence += next_char\n",
    "\n",
    "    print(sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michellezhao/anaconda3/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "48995/48995 [==============================] - 54s 1ms/step - loss: 2.9923\n",
      "Epoch 2/120\n",
      "48995/48995 [==============================] - 51s 1ms/step - loss: 2.3973\n",
      "Epoch 3/120\n",
      "48995/48995 [==============================] - 53s 1ms/step - loss: 2.2142\n",
      "Epoch 4/120\n",
      "48995/48995 [==============================] - 47s 967us/step - loss: 2.1181\n",
      "Epoch 5/120\n",
      "48995/48995 [==============================] - 46s 947us/step - loss: 2.0400\n",
      "Epoch 6/120\n",
      "48995/48995 [==============================] - 47s 950us/step - loss: 1.9842\n",
      "Epoch 7/120\n",
      "48995/48995 [==============================] - 47s 949us/step - loss: 1.9361\n",
      "Epoch 8/120\n",
      "48995/48995 [==============================] - 46s 946us/step - loss: 1.8982\n",
      "Epoch 9/120\n",
      "48995/48995 [==============================] - 47s 953us/step - loss: 1.8608\n",
      "Epoch 10/120\n",
      "48995/48995 [==============================] - 46s 946us/step - loss: 1.8299\n",
      "Epoch 11/120\n",
      "48995/48995 [==============================] - 47s 950us/step - loss: 1.7922\n",
      "Epoch 12/120\n",
      "48995/48995 [==============================] - 47s 950us/step - loss: 1.7667\n",
      "Epoch 13/120\n",
      "48995/48995 [==============================] - 46s 948us/step - loss: 1.7412\n",
      "Epoch 14/120\n",
      "48995/48995 [==============================] - 47s 949us/step - loss: 1.7161\n",
      "Epoch 15/120\n",
      "48995/48995 [==============================] - 47s 952us/step - loss: 1.6933\n",
      "Epoch 16/120\n",
      "48995/48995 [==============================] - 46s 949us/step - loss: 1.6703\n",
      "Epoch 17/120\n",
      "48995/48995 [==============================] - 46s 947us/step - loss: 1.6460\n",
      "Epoch 18/120\n",
      "48995/48995 [==============================] - 46s 947us/step - loss: 1.6223\n",
      "Epoch 19/120\n",
      "48995/48995 [==============================] - 46s 947us/step - loss: 1.6000\n",
      "Epoch 20/120\n",
      "48995/48995 [==============================] - 46s 948us/step - loss: 1.5837\n",
      "Epoch 21/120\n",
      "48995/48995 [==============================] - 46s 947us/step - loss: 1.5618\n",
      "Epoch 22/120\n",
      "48995/48995 [==============================] - 46s 948us/step - loss: 1.5445\n",
      "Epoch 23/120\n",
      "48995/48995 [==============================] - 47s 949us/step - loss: 1.5207\n",
      "Epoch 24/120\n",
      "48995/48995 [==============================] - 47s 949us/step - loss: 1.5001\n",
      "Epoch 25/120\n",
      "48995/48995 [==============================] - 46s 946us/step - loss: 1.4759\n",
      "Epoch 26/120\n",
      "48995/48995 [==============================] - 46s 948us/step - loss: 1.4539\n",
      "Epoch 27/120\n",
      "48995/48995 [==============================] - 46s 948us/step - loss: 1.4304\n",
      "Epoch 28/120\n",
      "48995/48995 [==============================] - 47s 950us/step - loss: 1.4068\n",
      "Epoch 29/120\n",
      "48995/48995 [==============================] - 47s 951us/step - loss: 1.3868\n",
      "Epoch 30/120\n",
      "48995/48995 [==============================] - 46s 945us/step - loss: 1.3639\n",
      "Epoch 31/120\n",
      "48995/48995 [==============================] - 46s 944us/step - loss: 1.3382\n",
      "Epoch 32/120\n",
      "48995/48995 [==============================] - 46s 946us/step - loss: 1.3130\n",
      "Epoch 33/120\n",
      "48995/48995 [==============================] - 48s 970us/step - loss: 1.2896\n",
      "Epoch 34/120\n",
      "48995/48995 [==============================] - 48s 988us/step - loss: 1.2669\n",
      "Epoch 35/120\n",
      "48995/48995 [==============================] - 49s 997us/step - loss: 1.2404\n",
      "Epoch 36/120\n",
      "48995/48995 [==============================] - 49s 994us/step - loss: 1.2158\n",
      "Epoch 37/120\n",
      "48995/48995 [==============================] - 49s 998us/step - loss: 1.1917\n",
      "Epoch 38/120\n",
      "48995/48995 [==============================] - 49s 995us/step - loss: 1.1681\n",
      "Epoch 39/120\n",
      "48995/48995 [==============================] - 49s 996us/step - loss: 1.1467\n",
      "Epoch 40/120\n",
      "48995/48995 [==============================] - 49s 995us/step - loss: 1.1181\n",
      "Epoch 41/120\n",
      "48995/48995 [==============================] - 49s 1000us/step - loss: 1.0970\n",
      "Epoch 42/120\n",
      "48995/48995 [==============================] - 49s 997us/step - loss: 1.0763\n",
      "Epoch 43/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 1.0521\n",
      "Epoch 44/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 1.0274\n",
      "Epoch 45/120\n",
      "48995/48995 [==============================] - 56s 1ms/step - loss: 1.0107\n",
      "Epoch 46/120\n",
      "48995/48995 [==============================] - 53s 1ms/step - loss: 0.9924\n",
      "Epoch 47/120\n",
      "48995/48995 [==============================] - 51s 1ms/step - loss: 0.9734\n",
      "Epoch 48/120\n",
      "48995/48995 [==============================] - 50s 1ms/step - loss: 0.9527\n",
      "Epoch 49/120\n",
      "48995/48995 [==============================] - 48s 975us/step - loss: 0.9370\n",
      "Epoch 50/120\n",
      "48995/48995 [==============================] - 47s 955us/step - loss: 0.9126\n",
      "Epoch 51/120\n",
      "48995/48995 [==============================] - 47s 952us/step - loss: 0.8950\n",
      "Epoch 52/120\n",
      "48995/48995 [==============================] - 47s 951us/step - loss: 0.8807\n",
      "Epoch 53/120\n",
      "48995/48995 [==============================] - 47s 956us/step - loss: 0.8628\n",
      "Epoch 54/120\n",
      "48995/48995 [==============================] - 49s 995us/step - loss: 0.8443\n",
      "Epoch 55/120\n",
      "48995/48995 [==============================] - 48s 982us/step - loss: 0.8293\n",
      "Epoch 56/120\n",
      "48995/48995 [==============================] - 48s 977us/step - loss: 0.8134\n",
      "Epoch 57/120\n",
      "48995/48995 [==============================] - 47s 967us/step - loss: 0.7966\n",
      "Epoch 58/120\n",
      "48995/48995 [==============================] - 47s 958us/step - loss: 0.7839\n",
      "Epoch 59/120\n",
      "48995/48995 [==============================] - 47s 964us/step - loss: 0.7743\n",
      "Epoch 60/120\n",
      "48995/48995 [==============================] - 47s 966us/step - loss: 0.7559\n",
      "Epoch 61/120\n",
      "48995/48995 [==============================] - 53s 1ms/step - loss: 0.7469\n",
      "Epoch 62/120\n",
      "48995/48995 [==============================] - 52s 1ms/step - loss: 0.7386\n",
      "Epoch 63/120\n",
      "48995/48995 [==============================] - 47s 957us/step - loss: 0.7225\n",
      "Epoch 64/120\n",
      "48995/48995 [==============================] - 49s 992us/step - loss: 0.7103\n",
      "Epoch 65/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.7054\n",
      "Epoch 66/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.6935\n",
      "Epoch 67/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.6839\n",
      "Epoch 68/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.6767\n",
      "Epoch 69/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.6672\n",
      "Epoch 70/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.6578\n",
      "Epoch 71/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.6502\n",
      "Epoch 72/120\n",
      "48995/48995 [==============================] - 50s 1ms/step - loss: 0.6401\n",
      "Epoch 73/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.6326\n",
      "Epoch 74/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.6209\n",
      "Epoch 75/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.6169\n",
      "Epoch 76/120\n",
      "48995/48995 [==============================] - 50s 1ms/step - loss: 0.6089\n",
      "Epoch 77/120\n",
      "48995/48995 [==============================] - 50s 1ms/step - loss: 0.6002\n",
      "Epoch 78/120\n",
      "48995/48995 [==============================] - 50s 1ms/step - loss: 0.6003\n",
      "Epoch 79/120\n",
      "48995/48995 [==============================] - 50s 1ms/step - loss: 0.5947\n",
      "Epoch 80/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.5835\n",
      "Epoch 81/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.5792\n",
      "Epoch 82/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.5737\n",
      "Epoch 83/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.5654\n",
      "Epoch 84/120\n",
      "48995/48995 [==============================] - 50s 1ms/step - loss: 0.5709\n",
      "Epoch 85/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.5592\n",
      "Epoch 86/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.5524\n",
      "Epoch 87/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.5442\n",
      "Epoch 88/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.5419\n",
      "Epoch 89/120\n",
      "48995/48995 [==============================] - 47s 958us/step - loss: 0.5320\n",
      "Epoch 90/120\n",
      "48995/48995 [==============================] - 47s 956us/step - loss: 0.5347\n",
      "Epoch 91/120\n",
      "48995/48995 [==============================] - 46s 948us/step - loss: 0.5261\n",
      "Epoch 92/120\n",
      "48995/48995 [==============================] - 46s 948us/step - loss: 0.5283\n",
      "Epoch 93/120\n",
      "48995/48995 [==============================] - 47s 951us/step - loss: 0.5160\n",
      "Epoch 94/120\n",
      "48995/48995 [==============================] - 46s 949us/step - loss: 0.5157\n",
      "Epoch 95/120\n",
      "48995/48995 [==============================] - 48s 979us/step - loss: 0.5168\n",
      "Epoch 96/120\n",
      "48995/48995 [==============================] - 48s 977us/step - loss: 0.5128\n",
      "Epoch 97/120\n",
      "48995/48995 [==============================] - 48s 979us/step - loss: 0.5066\n",
      "Epoch 98/120\n",
      "48995/48995 [==============================] - 48s 978us/step - loss: 0.5087\n",
      "Epoch 99/120\n",
      "48995/48995 [==============================] - 48s 980us/step - loss: 0.4897\n",
      "Epoch 100/120\n",
      "48995/48995 [==============================] - 48s 979us/step - loss: 0.4969\n",
      "Epoch 101/120\n",
      "48995/48995 [==============================] - 48s 983us/step - loss: 0.4884\n",
      "Epoch 102/120\n",
      "48995/48995 [==============================] - 48s 983us/step - loss: 0.4973\n",
      "Epoch 103/120\n",
      "48995/48995 [==============================] - 48s 980us/step - loss: 0.4875\n",
      "Epoch 104/120\n",
      "48995/48995 [==============================] - 48s 981us/step - loss: 0.4860\n",
      "Epoch 105/120\n",
      "48995/48995 [==============================] - 48s 981us/step - loss: 0.4849\n",
      "Epoch 106/120\n",
      "48995/48995 [==============================] - 48s 978us/step - loss: 0.4653\n",
      "Epoch 107/120\n",
      "48995/48995 [==============================] - 48s 980us/step - loss: 0.4633\n",
      "Epoch 108/120\n",
      "48995/48995 [==============================] - 48s 982us/step - loss: 0.4709\n",
      "Epoch 109/120\n",
      "48995/48995 [==============================] - 50s 1ms/step - loss: 0.4561\n",
      "Epoch 110/120\n",
      "48995/48995 [==============================] - 49s 990us/step - loss: 0.4590\n",
      "Epoch 111/120\n",
      "48995/48995 [==============================] - 49s 994us/step - loss: 0.4688\n",
      "Epoch 112/120\n",
      "48995/48995 [==============================] - 49s 994us/step - loss: 0.4589\n",
      "Epoch 113/120\n",
      "48995/48995 [==============================] - 49s 999us/step - loss: 0.4702\n",
      "Epoch 114/120\n",
      "48995/48995 [==============================] - 49s 994us/step - loss: 0.4567\n",
      "Epoch 115/120\n",
      "48995/48995 [==============================] - 55s 1ms/step - loss: 0.4504\n",
      "Epoch 116/120\n",
      "48995/48995 [==============================] - 53s 1ms/step - loss: 0.4503\n",
      "Epoch 117/120\n",
      "48995/48995 [==============================] - 50s 1ms/step - loss: 0.4543\n",
      "Epoch 118/120\n",
      "48995/48995 [==============================] - 49s 1ms/step - loss: 0.4532\n",
      "Epoch 119/120\n",
      "48995/48995 [==============================] - 52s 1ms/step - loss: 0.4276\n",
      "Epoch 120/120\n",
      "48995/48995 [==============================] - 50s 1ms/step - loss: 0.4322\n",
      "Testing Temperature of 1.5....\n",
      "shall i compare thee to a summer's day?\n",
      "Where of my liven such I have atteaking,\n",
      "How allity of love they miss, dewitedsush:\n",
      "O fittill grome deture thy crieks love's rep,\n",
      "O chanke she lought, ard for a dose wrese,\n",
      "This I concan to being mine one deeds.\n",
      "\n",
      "\n",
      "                   32\n",
      "Shate nat I could my love to despivey are,\n",
      "Leed by bainted of though to ghee so swent\n",
      "As sugs I in their port's vince wombed\n",
      "Inge, but you bose sight sillule sum ay shear,\n",
      "And sight thou art as wormed eyf hade:\n",
      "  From the gaire, mer as of troukh I singer,\n",
      "How mades har wrech-poaind chink deer mear,\n",
      "My resiming hasp, why dost thou mors are,\n",
      "Where ant my pussion that this mude decee,\n",
      "Which my form werrough nos undit so mud's on me,\n",
      "  Thet in the grest boon Musurs ald so claired,\n",
      "Thoush hawp heat steend a soow be blatter,\n",
      "Which shell muthard and to making his deemige,\n",
      "Yet not tauret a conlemed ad hour of crae,\n",
      "  Looked ansiev faich thine of los be faver,\n",
      "By anding time and thought eyes dotr sad,\n",
      "  You wisting less to neg all-gainded hade.\n",
      " \n",
      "Then of the tile\n",
      "\n",
      "\n",
      "Testing Temperature of 0.75....\n",
      "shall i compare thee to a summer's day?\n",
      "Then will but hears to, is not surw asthen,\n",
      "Which in the rear, and badnang lich now 'lan,\n",
      "Which of the hear, with thing amponsoume,\n",
      "But that when boty spony your spain,\n",
      "Whon worls I vilies leve I be chorno's diem.\n",
      "The cone thou artith of the saivilled,\n",
      "And but wheread I all ingress butt decoir,\n",
      "That heart I canket not len ulone of wind,\n",
      "But the preasude of thee,\n",
      "  an seees new it of suchives now redrant,\n",
      "Now stall be mise, love eaver you brefted,\n",
      "The part of hime, be fulling of your self brood,\n",
      "Beterce thou art could me world ruppiely dechise.\n",
      "\n",
      "To sise for lave is moth hea that thou shaser,\n",
      "That o'st withoul shases of ploasure on the hade,\n",
      "That you dost foom my doun indsting,\n",
      "Which it gut in time but infeeser warand.\n",
      "Thoult fay thou Mast hate gift of beauty lass,\n",
      "And 5hen my self swould beart it to gos.\n",
      "\n",
      "\n",
      "                   45\n",
      "Those sar meris beant that I loves forght,\n",
      "But my bastall be my amse the soll car mour:\n",
      "  For I an is thas swould bain thin mart,\n",
      "Hall the survair in offeccessed\n",
      "\n",
      "\n",
      "Testing Temperature of 0.25....\n",
      "shall i compare thee to a summer's day?\n",
      "Where of the have I storl; close but hath loves so floe a conle, for stongurs\n",
      "Cinlond bost did thus ackend mine one, read,\n",
      "Dedinge my longel of me,\n",
      "I love anowe thou bright their wandstidgen,\n",
      "That you winting thy propiou liff dith prage.\n",
      "\n",
      "\n",
      "                   145\n",
      "Those  with mine ow the krengress my not tongue:\n",
      "  For I must, what is is natue 'suns wher firs my deep,\n",
      "Than whoth time thou dost leas mine of thee,\n",
      "I still folfole sumbsts with unse my fair,\n",
      "On then core, hat ere me betticklings in shell\n",
      "Where-gan to end, blanty to to such cand,\n",
      "What being chownot poor that mance your's,\n",
      "Asd tho save ander thise my love to dser,\n",
      "That you how I spound pragse that his ew.\n",
      "I then foot form af I stoll withold,\n",
      "Nor live not to he men,redous hea tart lect,\n",
      "By that and ton upon thy self and treet,\n",
      "  When I am is not canded canding make,\n",
      "And eve I stone the bair of theme thou some:\n",
      "For gold wasces in aver-giaity,\n",
      "Or suen,\n",
      "Hor shather is made dight and thou hiss,\n",
      "Pition despece,\n",
      "And how old doting out\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(s_len, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#train the model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam')\n",
    "model.fit(trainX, trainY, batch_size = 128, nb_epoch = 120)\n",
    "\n",
    "print(\"Testing Temperature of 1.5....\")\n",
    "print_poem_temp(1000, 1.5, random = False)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Testing Temperature of 0.75....\")\n",
    "print_poem_temp(1000, 0.75, random = False)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Testing Temperature of 0.25....\")\n",
    "print_poem_temp(1000, 0.25, random = False)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
