{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import *\n",
    "from keras.preprocessing.text import *\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "from gensim.models import word2vec\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function from set 6 to parse the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_observations(text):\n",
    "    # Convert text to dataset.\n",
    "    lines = [line.split() for line in text.split('\\n') if line.split()]\n",
    "\n",
    "    obs_counter = 0\n",
    "    obs = []\n",
    "    obs_map = {}\n",
    "\n",
    "    for line in lines:\n",
    "        obs_elem = []\n",
    "        \n",
    "        for word in line:\n",
    "            word = re.sub(r'[^\\w]', '', word).lower()\n",
    "            if word not in obs_map:\n",
    "                # Add unique words to the observations map.\n",
    "                obs_map[word] = obs_counter\n",
    "                obs_counter += 1\n",
    "            \n",
    "            # Add the encoded word.\n",
    "            obs_elem.append(obs_map[word])\n",
    "        \n",
    "        # Add the encoded sequence.\n",
    "        obs.append(obs_elem)\n",
    "\n",
    "    return obs, obs_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = open(os.path.join(os.getcwd(), \\\n",
    "                         'Release/data/shakespeare.txt')).read()\n",
    "obs, obs_map = parse_observations(text)\n",
    "all_words = open(os.path.join(os.getcwd(), \\\n",
    "                         'Release/data/shakespeare.txt')).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chars = list(text)\n",
    "all_words = text.split()\n",
    "words = sorted(set(all_words))\n",
    "# print(len(words))\n",
    "# words2 = sorted(list(set(text.split())))\n",
    "# print(len(words2))\n",
    "chars = sorted(list(set(text)))\n",
    "data_size, vocab_size = len(text), len(chars)\n",
    "word_to_index = {char:idx for idx, char in enumerate(words)}\n",
    "index_to_word = {idx:char for idx, char in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 2\n",
    "s_len = 10\n",
    "training_data = []\n",
    "next_word = []\n",
    "for i in range(0, len(all_words)-s_len, n_step):\n",
    "    \n",
    "    string_to_add = all_words[i:i + s_len]\n",
    "    training_data.append(string_to_add)\n",
    "    if i + s_len < len(all_words)-1:\n",
    "        next_word.append(all_words[i + s_len])\n",
    "    \n",
    "# This is our end symbol: a space.\n",
    "#next_word.append('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8863\n",
      "8863\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))\n",
    "print(len(next_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.zeros((len(training_data), s_len, len(words)))\n",
    "trainY = np.zeros((len(training_data), len(words)))\n",
    "\n",
    "for sent_idx in range(0, len(training_data)):\n",
    "    curr_sentence = training_data[sent_idx]\n",
    "    curr_next_word = next_word[sent_idx]\n",
    "    for i, word in enumerate(curr_sentence):\n",
    "        trainX[sent_idx, i, word_to_index[word]] = 1\n",
    "    trainY[sent_idx, word_to_index[curr_next_word]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_train_data = []\n",
    "for data in training_data:\n",
    "    list_train_data.append(list(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49010\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))\n",
    "print(token_model_seqs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "(1, 49010, 10727)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "#text_tokens = tokenizer.texts_to_sequences(texts = list(training_data))\n",
    "tokenizer.fit_on_texts(list(training_data))\n",
    "token_model_seqs = tokenizer.texts_to_matrix(training_data)\n",
    "print(token_model_seqs)\n",
    "\n",
    "\n",
    "X = token_model_seqs.reshape(1, len(token_model_seqs), len(token_model_seqs[0]))\n",
    "y = tokenizer.texts_to_matrix(next_word)\n",
    "\n",
    "print(X.shape)\n",
    "# for sent_idx in range(0, len(training_data)):\n",
    "#     curr_sentence = training_data[sent_idx]\n",
    "#     curr_next_word = next_word[sent_idx]\n",
    "#     for i, word in enumerate(curr_sentence):\n",
    "#         trainX[sent_idx, i, word_to_index[word]] = 1\n",
    "#     trainY[sent_idx, word_to_index[curr_next_word]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = token_model_seqs\n",
    "y = tokenizer.texts_to_matrix(next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3350\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_poem_maxprob(length, random = False):\n",
    "    if random == True:\n",
    "        \n",
    "        start_index = np.random.randint(0, len(text.split())-s_len-1)\n",
    "        sentence1 = np.array(all_words[start_index: start_index + s_len])\n",
    "        \n",
    "    else:\n",
    "        sentence1 = np.array(['Shall', 'I', 'compare', 'thee', 'to', 'a', 'summer\\'s', 'day', 'Thou', 'art'])\n",
    "    #print(\"sentence\", sentence)\n",
    "    sentence = np.ndarray.tolist(sentence1)\n",
    "    sequence = np.ndarray.tolist(sentence1)\n",
    "    for i in range(length):\n",
    "        x_pred = np.zeros((1, s_len, len(words)))\n",
    "        for j in range(len(sentence)):\n",
    "            x_pred[0, j, word_to_index[sentence[j]]] = 1.\n",
    "\n",
    "        predictions = np.array(model.predict(x_pred)[0])\n",
    "        max_index = np.argmax(predictions)\n",
    "        next_word = index_to_word[max_index]\n",
    "        #print(next_word)\n",
    "        sentence.append(next_word)\n",
    "        sentence = sentence[1:]\n",
    "        sequence.append(next_word)\n",
    "    \n",
    "    #''.join(sequence)\n",
    "    output = ' '.join(str(x) for x in sequence)\n",
    "    print(output)\n",
    "    \n",
    "def print_poem_randprob(length, random = False):\n",
    "    if random == True:\n",
    "        \n",
    "        start_index = np.random.randint(0, len(text.split())-s_len-1)\n",
    "        sentence1 = np.array(all_words[start_index: start_index + s_len])\n",
    "        \n",
    "    else:\n",
    "        sentence1 = np.array(['Shall', 'I', 'compare', 'thee', 'to', 'a', 'summer\\'s', 'day', 'Thou', 'art'])\n",
    "        \n",
    "    sentence = np.ndarray.tolist(sentence1)\n",
    "    sequence = np.ndarray.tolist(sentence1)\n",
    "    for i in range(length):\n",
    "        x_pred = np.zeros((1, s_len, len(words)))\n",
    "        for j in range(len(sentence)):\n",
    "            x_pred[0, j, word_to_index[sentence[j]]] = 1.\n",
    "\n",
    "        predictions = np.array(model.predict(x_pred)[0])\n",
    "        index = np.random.choice(len(words), 1, p=predictions)[0]\n",
    "        max_index = np.argmax(predictions)\n",
    "        next_word = index_to_word[max_index]\n",
    "        #print(next_word)\n",
    "        sentence.append(next_word)\n",
    "        sentence = sentence[1:]\n",
    "        sequence.append(next_word)\n",
    "    \n",
    "    #''.join(sequence)\n",
    "    output = ' '.join(str(x) for x in sequence)\n",
    "    print(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michellezhao/anaconda3/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8863/8863 [==============================] - 74s 8ms/step - loss: 7.3512\n",
      "Epoch 2/100\n",
      "8863/8863 [==============================] - 64s 7ms/step - loss: 6.8719\n",
      "Epoch 3/100\n",
      "8863/8863 [==============================] - 64s 7ms/step - loss: 6.7830\n",
      "Epoch 4/100\n",
      "8863/8863 [==============================] - 62s 7ms/step - loss: 6.7201\n",
      "Epoch 5/100\n",
      "8863/8863 [==============================] - 61s 7ms/step - loss: 6.6787\n",
      "Epoch 6/100\n",
      "8863/8863 [==============================] - 61s 7ms/step - loss: 6.6429\n",
      "Epoch 7/100\n",
      "8863/8863 [==============================] - 61s 7ms/step - loss: 6.6219\n",
      "Epoch 8/100\n",
      "8863/8863 [==============================] - 61s 7ms/step - loss: 6.5934\n",
      "Epoch 9/100\n",
      "8863/8863 [==============================] - 63s 7ms/step - loss: 6.5648\n",
      "Epoch 10/100\n",
      "8863/8863 [==============================] - 61s 7ms/step - loss: 6.5368\n",
      "Epoch 11/100\n",
      "8863/8863 [==============================] - 63s 7ms/step - loss: 6.5068\n",
      "Epoch 12/100\n",
      "8863/8863 [==============================] - 65s 7ms/step - loss: 6.4561\n",
      "Epoch 13/100\n",
      "8863/8863 [==============================] - 61s 7ms/step - loss: 6.4054\n",
      "Epoch 14/100\n",
      "8863/8863 [==============================] - 61s 7ms/step - loss: 6.3503\n",
      "Epoch 15/100\n",
      "8863/8863 [==============================] - 62s 7ms/step - loss: 6.2847\n",
      "Epoch 16/100\n",
      "8863/8863 [==============================] - 65s 7ms/step - loss: 6.2026\n",
      "Epoch 17/100\n",
      "8863/8863 [==============================] - 62s 7ms/step - loss: 6.1141\n",
      "Epoch 18/100\n",
      "8863/8863 [==============================] - 250s 28ms/step - loss: 6.0130\n",
      "Epoch 19/100\n",
      "8863/8863 [==============================] - 87s 10ms/step - loss: 5.9205\n",
      "Epoch 20/100\n",
      "8863/8863 [==============================] - 52s 6ms/step - loss: 5.8245\n",
      "Epoch 21/100\n",
      "8863/8863 [==============================] - 48s 5ms/step - loss: 5.7255\n",
      "Epoch 22/100\n",
      "8863/8863 [==============================] - 48s 5ms/step - loss: 5.6303\n",
      "Epoch 23/100\n",
      "8863/8863 [==============================] - 46s 5ms/step - loss: 5.5311\n",
      "Epoch 24/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 5.4306\n",
      "Epoch 25/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 5.3349\n",
      "Epoch 26/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 5.2375\n",
      "Epoch 27/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 5.1327\n",
      "Epoch 28/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 5.0373\n",
      "Epoch 29/100\n",
      "8863/8863 [==============================] - 43s 5ms/step - loss: 4.9330\n",
      "Epoch 30/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 4.8508\n",
      "Epoch 31/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 4.7530\n",
      "Epoch 32/100\n",
      "8863/8863 [==============================] - 43s 5ms/step - loss: 4.6592\n",
      "Epoch 33/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 4.5604\n",
      "Epoch 34/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 4.4594\n",
      "Epoch 35/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 4.3828\n",
      "Epoch 36/100\n",
      "8863/8863 [==============================] - 43s 5ms/step - loss: 4.2861\n",
      "Epoch 37/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 4.1839\n",
      "Epoch 38/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 4.1006\n",
      "Epoch 39/100\n",
      "8863/8863 [==============================] - 43s 5ms/step - loss: 4.0084\n",
      "Epoch 40/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 3.9125\n",
      "Epoch 41/100\n",
      "8863/8863 [==============================] - 43s 5ms/step - loss: 3.8501\n",
      "Epoch 42/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 3.7455\n",
      "Epoch 43/100\n",
      "8863/8863 [==============================] - 45s 5ms/step - loss: 3.6576\n",
      "Epoch 44/100\n",
      "8863/8863 [==============================] - 46s 5ms/step - loss: 3.5813\n",
      "Epoch 45/100\n",
      "8863/8863 [==============================] - 43s 5ms/step - loss: 3.4858\n",
      "Epoch 46/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 3.4176\n",
      "Epoch 47/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 3.3296\n",
      "Epoch 48/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 3.2550\n",
      "Epoch 49/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 3.1691\n",
      "Epoch 50/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 3.1031\n",
      "Epoch 51/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 3.0303\n",
      "Epoch 52/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 2.9438\n",
      "Epoch 53/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 2.8720\n",
      "Epoch 54/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 2.7993\n",
      "Epoch 55/100\n",
      "8863/8863 [==============================] - 43s 5ms/step - loss: 2.7165\n",
      "Epoch 56/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 2.6536\n",
      "Epoch 57/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 2.5784\n",
      "Epoch 58/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 2.5122\n",
      "Epoch 59/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 2.4460\n",
      "Epoch 60/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 2.3810\n",
      "Epoch 61/100\n",
      "8863/8863 [==============================] - 43s 5ms/step - loss: 2.3407\n",
      "Epoch 62/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 2.2871\n",
      "Epoch 63/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 2.2341\n",
      "Epoch 64/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 2.1843\n",
      "Epoch 65/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 2.1488\n",
      "Epoch 66/100\n",
      "8863/8863 [==============================] - 43s 5ms/step - loss: 2.1000\n",
      "Epoch 67/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 2.0499\n",
      "Epoch 68/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 2.0085\n",
      "Epoch 69/100\n",
      "8863/8863 [==============================] - 43s 5ms/step - loss: 1.9552\n",
      "Epoch 70/100\n",
      "8863/8863 [==============================] - 43s 5ms/step - loss: 1.9287\n",
      "Epoch 71/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 1.8861\n",
      "Epoch 72/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 1.8529\n",
      "Epoch 73/100\n",
      "8863/8863 [==============================] - 43s 5ms/step - loss: 1.8107\n",
      "Epoch 74/100\n",
      "8863/8863 [==============================] - 44s 5ms/step - loss: 1.7712\n",
      "Epoch 75/100\n",
      "8863/8863 [==============================] - 42s 5ms/step - loss: 1.7453\n",
      "Epoch 76/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.7126\n",
      "Epoch 77/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.6600\n",
      "Epoch 78/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.6247\n",
      "Epoch 79/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.5974\n",
      "Epoch 80/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.5762\n",
      "Epoch 81/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.5365\n",
      "Epoch 82/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.5135\n",
      "Epoch 83/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.4848\n",
      "Epoch 84/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.4692\n",
      "Epoch 85/100\n",
      "8863/8863 [==============================] - 42s 5ms/step - loss: 1.4341\n",
      "Epoch 86/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.4173\n",
      "Epoch 87/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.3813\n",
      "Epoch 88/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.3666\n",
      "Epoch 89/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.3505\n",
      "Epoch 90/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.3270\n",
      "Epoch 91/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.2993\n",
      "Epoch 92/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.2875\n",
      "Epoch 93/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.2681\n",
      "Epoch 94/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.2378\n",
      "Epoch 95/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.2087\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.2076\n",
      "Epoch 97/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.1855\n",
      "Epoch 98/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.1690\n",
      "Epoch 99/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.1590\n",
      "Epoch 100/100\n",
      "8863/8863 [==============================] - 41s 5ms/step - loss: 1.1364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4785c278>"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(s_len, len(words))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(words)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#train the model\n",
    "\n",
    "#print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop')\n",
    "model.fit(trainX, trainY, batch_size = 128, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall I compare thee to a summer's day Thou art more more and my with too with This dost dost thy O it from So When So is yet much had The Not can so so numbers things hold do from a to to That to fair for As the left him him losing seeming his beauty beauty beauty have have he those have in with you you with am though though him Time bars wasteful change love's him change change he true their day let I are than hast the I to in mourn should loss blame me my in in I which I better in your mine with\n",
      "end\n",
      "\n",
      "Shall I compare thee to a summer's day Thou art more more and my with too with This dost dost thy O it from So When So is yet much had The Not can so so numbers things hold do from a to to That to fair for As the left him him losing seeming his beauty beauty beauty have have he those have in with you you with am though though him Time bars wasteful change love's him change change he true their day let I are than hast the I to in mourn should loss blame me my in in I which I better in your mine with\n"
     ]
    }
   ],
   "source": [
    "print_poem_maxprob(100, False)\n",
    "print(\"end\\n\")\n",
    "\n",
    "print_poem_randprob(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
