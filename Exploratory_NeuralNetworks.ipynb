{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, SimpleRNN, GRU, Bidirectional\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function from set 6 to parse the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_observations(text):\n",
    "    # Convert text to dataset.\n",
    "    lines = [line.split() for line in text.split('\\n') if line.split()]\n",
    "\n",
    "    obs_counter = 0\n",
    "    obs = []\n",
    "    obs_map = {}\n",
    "\n",
    "    for line in lines:\n",
    "        obs_elem = []\n",
    "        \n",
    "        for word in line:\n",
    "            word = re.sub(r'[^\\w]', '', word).lower()\n",
    "            if word not in obs_map:\n",
    "                # Add unique words to the observations map.\n",
    "                obs_map[word] = obs_counter\n",
    "                obs_counter += 1\n",
    "            \n",
    "            # Add the encoded word.\n",
    "            obs_elem.append(obs_map[word])\n",
    "        \n",
    "        # Add the encoded sequence.\n",
    "        obs.append(obs_elem)\n",
    "\n",
    "    return obs, obs_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = open(os.path.join(os.getcwd(), \\\n",
    "                         'Release/data/shakespeare.txt')).read()\n",
    "obs, obs_map = parse_observations(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we’re doing a character-based LSTM, first, we create a sorted dictionary of all characters used in the entire text, where each character is assigned an index. We create a two dictionaries, a character-to-index dictionary and an index-to-character dictionary, which will be used later for easy access to characters and indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chars = list(text)\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {char:idx for idx, char in enumerate(chars)}\n",
    "index_to_char = {idx:char for idx, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We break up the text into 40-character-length sequences, which will be our training X’s. For each 40 character sequence, the corresponding training Y will be the next character that follows. These input pairs will be the data that we train the LSTM on. In our code, we can also specify a step size (n_step) so that we can take the 40-character sequences at different intervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_step = 2\n",
    "s_len = 40\n",
    "training_data = []\n",
    "next_char = []\n",
    "for i in range(0, len(text)-s_len, n_step):\n",
    "    string_to_add = text[i:i + s_len]\n",
    "    training_data.append(string_to_add)\n",
    "    if i + s_len < len(text)-1:\n",
    "        next_char.append(text[i + s_len])\n",
    "    \n",
    "# This is our end symbol: a space\n",
    "next_char.append(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras LSTM takes a 3-dimensional input for X and a 2-dimensional input for Y. The dimensions of the X input are [number of sequences x sequence length(40) x length of character vector]. The dimensions of Y are [number of sequences x length of character vector]. The X-input is the list of 40-character sequences, where each position contains the vectorized character. The Y-input is the list of characters following the 40-char sequences, where each character is also vectorized. We manually 1-hot vectorize the characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX = np.zeros((len(training_data), s_len, len(chars)))\n",
    "trainY = np.zeros((len(training_data), len(chars)))\n",
    "\n",
    "for sent_idx in range(0, len(training_data)):\n",
    "    curr_sentence = training_data[sent_idx]\n",
    "    curr_next_char = next_char[sent_idx]\n",
    "    for i, char in enumerate(curr_sentence):\n",
    "        trainX[sent_idx, i, char_to_index[char]] = 1\n",
    "    trainY[sent_idx, char_to_index[curr_next_char]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for printing out poems from this LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_poem_maxprob(length, random = False):\n",
    "    if random == True:\n",
    "        start_index = np.random.randint(0, len(text)-s_len-1)\n",
    "        start_index = 50\n",
    "        sentence = text[start_index: start_index + s_len]\n",
    "    else:\n",
    "        sentence = \"shall i compare thee to a summer's day?\\n\"\n",
    "    sequence = sentence\n",
    "    for i in range(length):\n",
    "        x_pred = np.zeros((1, s_len, len(chars)))\n",
    "        for j, char in enumerate(sentence):\n",
    "            x_pred[0, j, char_to_index[char]] = 1.\n",
    "\n",
    "        predictions = np.array(model.predict(x_pred)[0])\n",
    "        max_index = np.argmax(predictions)\n",
    "        next_char = index_to_char[max_index]\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sequence += next_char\n",
    "\n",
    "    print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_poem_randprob(length, random = False):\n",
    "    if random == True:\n",
    "        start_index = np.random.randint(0, len(text)-s_len-1)\n",
    "        start_index = 50\n",
    "        sentence = text[start_index: start_index + s_len]\n",
    "    else:\n",
    "        sentence = \"shall i compare thee to a summer's day?\\n\"\n",
    "    sequence = sentence\n",
    "    for i in range(length):\n",
    "        x_pred = np.zeros((1, s_len, len(chars)))\n",
    "        for j, char in enumerate(sentence):\n",
    "            x_pred[0, j, char_to_index[char]] = 1.\n",
    "\n",
    "        predictions = np.array(model.predict(x_pred)[0])\n",
    "        index = np.random.choice(len(chars), 1, p=predictions)[0]\n",
    "        next_char = index_to_char[index]\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sequence += next_char\n",
    "\n",
    "    print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we use a bidirectional LSTM layers of 120 units, with a dropout of 0.2. It has a standard fully-connected output layer with a softmax activation. We train it for 40 epochs. We print out the poems generated by selecting the maximum probability letter and also the poems generated by selecting a random character weighted based on the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michellezhao/anaconda3/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "48995/48995 [==============================] - 53s 1ms/step - loss: 2.9191\n",
      "Epoch 2/40\n",
      "48995/48995 [==============================] - 46s 943us/step - loss: 2.4056\n",
      "Epoch 3/40\n",
      "48995/48995 [==============================] - 43s 885us/step - loss: 2.2049\n",
      "Epoch 4/40\n",
      "48995/48995 [==============================] - 44s 904us/step - loss: 2.1050\n",
      "Epoch 5/40\n",
      "48995/48995 [==============================] - 43s 885us/step - loss: 2.0367\n",
      "Epoch 6/40\n",
      "48995/48995 [==============================] - 43s 884us/step - loss: 1.9781\n",
      "Epoch 7/40\n",
      "48995/48995 [==============================] - 43s 885us/step - loss: 1.9314\n",
      "Epoch 8/40\n",
      "48995/48995 [==============================] - 43s 886us/step - loss: 1.8908\n",
      "Epoch 9/40\n",
      "48995/48995 [==============================] - 43s 883us/step - loss: 1.8577\n",
      "Epoch 10/40\n",
      "48995/48995 [==============================] - 44s 892us/step - loss: 1.8254\n",
      "Epoch 11/40\n",
      "48995/48995 [==============================] - 44s 889us/step - loss: 1.7969\n",
      "Epoch 12/40\n",
      "48995/48995 [==============================] - 44s 891us/step - loss: 1.7698\n",
      "Epoch 13/40\n",
      "48995/48995 [==============================] - 44s 895us/step - loss: 1.7451\n",
      "Epoch 14/40\n",
      "48995/48995 [==============================] - 44s 898us/step - loss: 1.7201\n",
      "Epoch 15/40\n",
      "48995/48995 [==============================] - 44s 901us/step - loss: 1.6973\n",
      "Epoch 16/40\n",
      "48995/48995 [==============================] - 44s 901us/step - loss: 1.6790\n",
      "Epoch 17/40\n",
      "48995/48995 [==============================] - 44s 903us/step - loss: 1.6555\n",
      "Epoch 18/40\n",
      "48995/48995 [==============================] - 44s 893us/step - loss: 1.6350\n",
      "Epoch 19/40\n",
      "48995/48995 [==============================] - 44s 899us/step - loss: 1.6116\n",
      "Epoch 20/40\n",
      "48995/48995 [==============================] - 44s 899us/step - loss: 1.5937\n",
      "Epoch 21/40\n",
      "48995/48995 [==============================] - 44s 905us/step - loss: 1.5739\n",
      "Epoch 22/40\n",
      "48995/48995 [==============================] - 45s 910us/step - loss: 1.5508\n",
      "Epoch 23/40\n",
      "48995/48995 [==============================] - 45s 917us/step - loss: 1.5355\n",
      "Epoch 24/40\n",
      "48995/48995 [==============================] - 45s 917us/step - loss: 1.5160\n",
      "Epoch 25/40\n",
      "48995/48995 [==============================] - 45s 917us/step - loss: 1.4921\n",
      "Epoch 26/40\n",
      "48995/48995 [==============================] - 45s 918us/step - loss: 1.4739\n",
      "Epoch 27/40\n",
      "48995/48995 [==============================] - 45s 922us/step - loss: 1.4546\n",
      "Epoch 28/40\n",
      "48995/48995 [==============================] - 45s 925us/step - loss: 1.4292\n",
      "Epoch 29/40\n",
      "48995/48995 [==============================] - 45s 925us/step - loss: 1.4125\n",
      "Epoch 30/40\n",
      "48995/48995 [==============================] - 47s 958us/step - loss: 1.3893\n",
      "Epoch 31/40\n",
      "48995/48995 [==============================] - 47s 964us/step - loss: 1.3637\n",
      "Epoch 32/40\n",
      "48995/48995 [==============================] - 46s 944us/step - loss: 1.3487\n",
      "Epoch 33/40\n",
      "48995/48995 [==============================] - 45s 923us/step - loss: 1.3251\n",
      "Epoch 34/40\n",
      "48995/48995 [==============================] - 46s 944us/step - loss: 1.3010\n",
      "Epoch 35/40\n",
      "48995/48995 [==============================] - 45s 925us/step - loss: 1.2786\n",
      "Epoch 36/40\n",
      "48995/48995 [==============================] - 46s 930us/step - loss: 1.2633\n",
      "Epoch 37/40\n",
      "48995/48995 [==============================] - 46s 929us/step - loss: 1.2381\n",
      "Epoch 38/40\n",
      "48995/48995 [==============================] - 45s 927us/step - loss: 1.2163\n",
      "Epoch 39/40\n",
      "48995/48995 [==============================] - 45s 928us/step - loss: 1.1926\n",
      "Epoch 40/40\n",
      "48995/48995 [==============================] - 45s 928us/step - loss: 1.1655\n",
      "shall i compare thee to a summer's day?\n",
      "  That my self thou shell will grow not hear,\n",
      "Then the rought of memy thou must that bled,\n",
      "That thou make the mort of thime thou art,\n",
      "That which I thee star thou doth should bear,\n",
      "Which I my self and love with thee world,\n",
      "But thes with though shall beauty the cain,\n",
      "And you stoull grownd will say be theee,\n",
      "  Thes be attere thee the earth and soust,\n",
      "And beauty themest thou mort be the self to be,\n",
      "  Thing the sheming that that I make mand beart,\n",
      "So love to the wird world with thou dost blend,\n",
      "Beart the rove thou thy still in the word.\n",
      "  Thes fair thy self art thing why some thee storl,\n",
      "But the reate thou thy bestarnd of your stare,\n",
      "But the foulds in shear with sead,\n",
      "And tome thou some thy she thy self that her,\n",
      "That which thou dott thou dost thou shall bear,\n",
      "  The carstt of your this fair that mes thee.\n",
      "\n",
      "\n",
      "                    11\n",
      "\n",
      "      : \n",
      "   s chow,\n",
      "  I have my merst and that the mind on thee,\n",
      "Which I she love with thee that heart the rays,\n",
      "The looks on thy striess with thou doth say.\n",
      "\n",
      "\n",
      "  \n",
      "shall i compare thee to a summer's day?\n",
      "And mary by coll deas thou that mence,\n",
      "Where of thy small, ard fom in priese hase,\n",
      "My death to thceenhath wlice to kelf to thee,\n",
      "  Bet in I love in this bath blay I dame,\n",
      "My homeswend doan that stide how shael hot,\n",
      "Say his deast one 'etme thes maiderach is hos thime,\n",
      "  At thy ayose thee eargh troughtr spoind,\n",
      "When it efters confaimet which sull look sinv,\n",
      "Than verse not a proamese hey the bonce,\n",
      "So live that thy pross 8him made constingd\n",
      "Ang your sweet swelt fall pised thy wild,\n",
      "'rAnch in my love a fair in the efvengllige,\n",
      "With must onclepurningeg sais bratce thoue,\n",
      "Hall sis well his ngwered that thou dost geld,\n",
      "And mund thy sworl inge plack thou whing gleased,\n",
      "Whach if is your sulesting his thine ris bload,\n",
      "O may which my mind, bllows love move to be,\n",
      "  by will gutiever is hade,\n",
      "ihs those thear sunjuts no truemping sout,\n",
      "And well and mage angouth side till tane,\n",
      "When they nit I core ar gave thy death deciel,\n",
      "Beavingh my sout zindigh diy baingat choom,\n",
      "n Musy that when I when my loss w\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(120),\n",
    "                        input_shape=(s_len, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#train the model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop')\n",
    "model.fit(trainX, trainY, batch_size = 128, nb_epoch = 40)\n",
    "print_poem_maxprob(1000)\n",
    "print_poem_randprob(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we train a simple RNN layer with 100 units, with a dropout of 0.2. It has a standard fully-connected output layer with a softmax activation. We train it for 50 epochs. We print out the poems generated by selecting the maximum probability letter and also the poems generated by selecting a random character weighted based on the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michellezhao/anaconda3/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "48995/48995 [==============================] - 9s 191us/step - loss: 2.8478\n",
      "Epoch 2/50\n",
      "48995/48995 [==============================] - 8s 170us/step - loss: 2.3959\n",
      "Epoch 3/50\n",
      "48995/48995 [==============================] - 8s 171us/step - loss: 2.2499\n",
      "Epoch 4/50\n",
      "48995/48995 [==============================] - 8s 171us/step - loss: 2.1795\n",
      "Epoch 5/50\n",
      "48995/48995 [==============================] - 8s 167us/step - loss: 2.1339\n",
      "Epoch 6/50\n",
      "48995/48995 [==============================] - 8s 166us/step - loss: 2.1048\n",
      "Epoch 7/50\n",
      "48995/48995 [==============================] - 8s 167us/step - loss: 2.0828\n",
      "Epoch 8/50\n",
      "48995/48995 [==============================] - 8s 169us/step - loss: 2.0655\n",
      "Epoch 9/50\n",
      "48995/48995 [==============================] - 8s 166us/step - loss: 2.0516\n",
      "Epoch 10/50\n",
      "48995/48995 [==============================] - 8s 167us/step - loss: 2.0388\n",
      "Epoch 11/50\n",
      "48995/48995 [==============================] - 8s 167us/step - loss: 2.0274\n",
      "Epoch 12/50\n",
      "48995/48995 [==============================] - 8s 167us/step - loss: 2.0145\n",
      "Epoch 13/50\n",
      "48995/48995 [==============================] - 8s 167us/step - loss: 2.0022\n",
      "Epoch 14/50\n",
      "48995/48995 [==============================] - 8s 168us/step - loss: 1.9913\n",
      "Epoch 15/50\n",
      "48995/48995 [==============================] - 8s 168us/step - loss: 1.9843\n",
      "Epoch 16/50\n",
      "48995/48995 [==============================] - 8s 168us/step - loss: 1.9736\n",
      "Epoch 17/50\n",
      "48995/48995 [==============================] - 8s 168us/step - loss: 1.9663 0s \n",
      "Epoch 18/50\n",
      "48995/48995 [==============================] - 8s 169us/step - loss: 1.9576\n",
      "Epoch 19/50\n",
      "48995/48995 [==============================] - 8s 169us/step - loss: 1.9459\n",
      "Epoch 20/50\n",
      "48995/48995 [==============================] - 8s 168us/step - loss: 1.9378\n",
      "Epoch 21/50\n",
      "48995/48995 [==============================] - 8s 169us/step - loss: 1.9330\n",
      "Epoch 22/50\n",
      "48995/48995 [==============================] - 8s 172us/step - loss: 1.9247\n",
      "Epoch 23/50\n",
      "48995/48995 [==============================] - 8s 172us/step - loss: 1.9195\n",
      "Epoch 24/50\n",
      "48995/48995 [==============================] - 8s 169us/step - loss: 1.9099\n",
      "Epoch 25/50\n",
      "48995/48995 [==============================] - 8s 169us/step - loss: 1.9020\n",
      "Epoch 26/50\n",
      "48995/48995 [==============================] - 8s 172us/step - loss: 1.8983\n",
      "Epoch 27/50\n",
      "48995/48995 [==============================] - 8s 171us/step - loss: 1.8923\n",
      "Epoch 28/50\n",
      "48995/48995 [==============================] - 8s 173us/step - loss: 1.8886\n",
      "Epoch 29/50\n",
      "48995/48995 [==============================] - 9s 174us/step - loss: 1.8792\n",
      "Epoch 30/50\n",
      "48995/48995 [==============================] - 9s 175us/step - loss: 1.8741\n",
      "Epoch 31/50\n",
      "48995/48995 [==============================] - 9s 180us/step - loss: 1.8685\n",
      "Epoch 32/50\n",
      "48995/48995 [==============================] - 9s 174us/step - loss: 1.8629\n",
      "Epoch 33/50\n",
      "48995/48995 [==============================] - 8s 172us/step - loss: 1.8557\n",
      "Epoch 34/50\n",
      "48995/48995 [==============================] - 8s 172us/step - loss: 1.8505\n",
      "Epoch 35/50\n",
      "48995/48995 [==============================] - 9s 174us/step - loss: 1.8469\n",
      "Epoch 36/50\n",
      "48995/48995 [==============================] - 8s 172us/step - loss: 1.8442\n",
      "Epoch 37/50\n",
      "48995/48995 [==============================] - 9s 176us/step - loss: 1.8339\n",
      "Epoch 38/50\n",
      "48995/48995 [==============================] - 8s 172us/step - loss: 1.8298 1s - ETA: 0s - loss: 1\n",
      "Epoch 39/50\n",
      "48995/48995 [==============================] - 9s 177us/step - loss: 1.8291\n",
      "Epoch 40/50\n",
      "48995/48995 [==============================] - 9s 176us/step - loss: 1.8216\n",
      "Epoch 41/50\n",
      "48995/48995 [==============================] - 9s 174us/step - loss: 1.8187 0s - loss: 1\n",
      "Epoch 42/50\n",
      "48995/48995 [==============================] - 9s 176us/step - loss: 1.8126\n",
      "Epoch 43/50\n",
      "48995/48995 [==============================] - 9s 178us/step - loss: 1.8055\n",
      "Epoch 44/50\n",
      "48995/48995 [==============================] - 9s 176us/step - loss: 1.8058  - ETA: 2s  -  - ETA: 0s - loss: 1.80\n",
      "Epoch 45/50\n",
      "48995/48995 [==============================] - 9s 177us/step - loss: 1.7984 1s - loss - ETA: 1s - loss - ETA: 0s - lo\n",
      "Epoch 46/50\n",
      "48995/48995 [==============================] - 9s 177us/step - loss: 1.7951\n",
      "Epoch 47/50\n",
      "48995/48995 [==============================] - 9s 176us/step - loss: 1.7913\n",
      "Epoch 48/50\n",
      "48995/48995 [==============================] - 9s 182us/step - loss: 1.7867\n",
      "Epoch 49/50\n",
      "48995/48995 [==============================] - 9s 176us/step - loss: 1.7842\n",
      "Epoch 50/50\n",
      "48995/48995 [==============================] - 10s 202us/step - loss: 1.7794\n",
      "shall i compare thee to a summer's day?\n",
      "When I the will che the serfich the beauty,\n",
      "The will che the serfich the beauty the sure,\n",
      "The will the beauty the suren the sur thee,\n",
      "When thou brain that the the beauty the suren, and thee,\n",
      "The will conder the she thee the sear she thee,\n",
      "  The will the beauty the suren the sur thee,\n",
      "When thou brain that the the beauty the suren, and thee,\n",
      "The will conder the she thee the sear she thee,\n",
      "  The will the beauty the suren the sur thee,\n",
      "When thou brain that the the beauty the suren, and thee,\n",
      "The will conder the she thee the sear she thee,\n",
      "  The will the beauty the suren the sur thee,\n",
      "When thou brain that the the beauty the suren, and thee,\n",
      "The will conder the she thee the sear she thee,\n",
      "  The will the beauty the suren the sur thee,\n",
      "When thou brain that the the beauty the suren, and thee,\n",
      "The will conder the she thee the sear she thee,\n",
      "  The will the beauty the suren the sur thee,\n",
      "When thou brain that the the beauty the suren, and thee,\n",
      "The will conder the she thee the sear she thee,\n",
      "  The \n",
      "shall i compare thee to a summer's day?\n",
      "I sumy yourlf that the belating tuel ptadis Ifer,\n",
      "The mowey but my kalt in the cores thees:\n",
      "-sant  werimy tonceed beathy to lefs miry.\n",
      "Then enon shimy fioth shathed in thing attare\n",
      "I se hor beay reage, that in lay, n ty pbyrung:\n",
      "But that nowirs oftenOfrs bolcamy farth eare,\n",
      "Of in my I hisllald stariast thee arrst ance,\n",
      "I  all me sild reon mite, thees roce, thy stold thes,\n",
      "Brand brecostButl crelofos ghowound as riltppu\n",
      "n bans, tho furt il duther is me,\n",
      "The veruga you shall canleirissa'e of cound.\n",
      "Buth s of yoke ere ter anl you his thite in my ara\n",
      "gSos all I are tios rus jyest:\n",
      "Not sd leem, wrich w'y fore love in thee ans,\n",
      "Most hinwinou wiet, that my dain feer me and 10(\n",
      "Whye vereain my self chast it the to show,\n",
      "If seas crecistlratcay wime this roven ingare:\n",
      ") Sh ste ruthins ay hom my stehes be if then mbare noms rives b,\n",
      "Bet and apteale bio  heit cholder pind\n",
      "m Whole urshe it in manted, and send thour',\n",
      "And that is this fraige thy uedaliethil ail Ifen.\n",
      "A-douthtint it in lisecule tfad h\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(100, input_shape=(s_len, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#train the model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop')\n",
    "model.fit(trainX, trainY, batch_size = 128, nb_epoch = 50)\n",
    "print_poem_maxprob(1000)\n",
    "print_poem_randprob(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we use a GRU recurrent network layer of 120 units, with a dropout of 0.2. It has a standard fully-connected output layer with a softmax activation. We train it for 50 epochs. We print out the poems generated by selecting the maximum probability letter and also the poems generated by selecting a random character weighted based on the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michellezhao/anaconda3/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "48995/48995 [==============================] - 26s 533us/step - loss: 2.8573\n",
      "Epoch 2/50\n",
      "48995/48995 [==============================] - 23s 470us/step - loss: 2.2418\n",
      "Epoch 3/50\n",
      "48995/48995 [==============================] - 23s 470us/step - loss: 2.0789\n",
      "Epoch 4/50\n",
      "48995/48995 [==============================] - 23s 469us/step - loss: 1.9884\n",
      "Epoch 5/50\n",
      "48995/48995 [==============================] - 23s 471us/step - loss: 1.9224\n",
      "Epoch 6/50\n",
      "48995/48995 [==============================] - 23s 476us/step - loss: 1.8718\n",
      "Epoch 7/50\n",
      "48995/48995 [==============================] - 23s 470us/step - loss: 1.8282\n",
      "Epoch 8/50\n",
      "48995/48995 [==============================] - 23s 472us/step - loss: 1.7871\n",
      "Epoch 9/50\n",
      "48995/48995 [==============================] - 23s 475us/step - loss: 1.7538\n",
      "Epoch 10/50\n",
      "48995/48995 [==============================] - 23s 476us/step - loss: 1.7236\n",
      "Epoch 11/50\n",
      "48995/48995 [==============================] - 23s 476us/step - loss: 1.6965\n",
      "Epoch 12/50\n",
      "48995/48995 [==============================] - 23s 477us/step - loss: 1.6741\n",
      "Epoch 13/50\n",
      "48995/48995 [==============================] - 23s 476us/step - loss: 1.6484\n",
      "Epoch 14/50\n",
      "48995/48995 [==============================] - 23s 477us/step - loss: 1.6300\n",
      "Epoch 15/50\n",
      "48995/48995 [==============================] - 24s 481us/step - loss: 1.6053\n",
      "Epoch 16/50\n",
      "48995/48995 [==============================] - 26s 535us/step - loss: 1.5871\n",
      "Epoch 17/50\n",
      "48995/48995 [==============================] - 24s 485us/step - loss: 1.5677\n",
      "Epoch 18/50\n",
      "48995/48995 [==============================] - 22s 445us/step - loss: 1.5473\n",
      "Epoch 19/50\n",
      "48995/48995 [==============================] - 23s 466us/step - loss: 1.5277\n",
      "Epoch 20/50\n",
      "48995/48995 [==============================] - 24s 484us/step - loss: 1.5116\n",
      "Epoch 21/50\n",
      "48995/48995 [==============================] - 23s 471us/step - loss: 1.4885\n",
      "Epoch 22/50\n",
      "48995/48995 [==============================] - 23s 471us/step - loss: 1.4741\n",
      "Epoch 23/50\n",
      "48995/48995 [==============================] - 23s 471us/step - loss: 1.4507\n",
      "Epoch 24/50\n",
      "48995/48995 [==============================] - 24s 489us/step - loss: 1.4352\n",
      "Epoch 25/50\n",
      "48995/48995 [==============================] - 24s 487us/step - loss: 1.4123\n",
      "Epoch 26/50\n",
      "48995/48995 [==============================] - 22s 456us/step - loss: 1.3973\n",
      "Epoch 27/50\n",
      "48995/48995 [==============================] - 23s 476us/step - loss: 1.3811\n",
      "Epoch 28/50\n",
      "48995/48995 [==============================] - 22s 455us/step - loss: 1.3641\n",
      "Epoch 29/50\n",
      "48995/48995 [==============================] - 22s 454us/step - loss: 1.3447\n",
      "Epoch 30/50\n",
      "48995/48995 [==============================] - 22s 445us/step - loss: 1.3269\n",
      "Epoch 31/50\n",
      "48995/48995 [==============================] - 22s 446us/step - loss: 1.3114\n",
      "Epoch 32/50\n",
      "48995/48995 [==============================] - 22s 448us/step - loss: 1.2940\n",
      "Epoch 33/50\n",
      "48995/48995 [==============================] - 22s 445us/step - loss: 1.2797\n",
      "Epoch 34/50\n",
      "48995/48995 [==============================] - 22s 443us/step - loss: 1.2633\n",
      "Epoch 35/50\n",
      "48995/48995 [==============================] - 22s 443us/step - loss: 1.2448\n",
      "Epoch 36/50\n",
      "48995/48995 [==============================] - 22s 444us/step - loss: 1.2322\n",
      "Epoch 37/50\n",
      "48995/48995 [==============================] - 22s 443us/step - loss: 1.2172\n",
      "Epoch 38/50\n",
      "48995/48995 [==============================] - 22s 444us/step - loss: 1.2005\n",
      "Epoch 39/50\n",
      "48995/48995 [==============================] - 22s 456us/step - loss: 1.1915\n",
      "Epoch 40/50\n",
      "48995/48995 [==============================] - 22s 445us/step - loss: 1.1748\n",
      "Epoch 41/50\n",
      "48995/48995 [==============================] - 22s 445us/step - loss: 1.1606\n",
      "Epoch 42/50\n",
      "48995/48995 [==============================] - 23s 470us/step - loss: 1.1471\n",
      "Epoch 43/50\n",
      "48995/48995 [==============================] - 23s 475us/step - loss: 1.1447\n",
      "Epoch 44/50\n",
      "48995/48995 [==============================] - 24s 492us/step - loss: 1.1307\n",
      "Epoch 45/50\n",
      "48995/48995 [==============================] - 27s 559us/step - loss: 1.1135\n",
      "Epoch 46/50\n",
      "48995/48995 [==============================] - 24s 499us/step - loss: 1.1133\n",
      "Epoch 47/50\n",
      "48995/48995 [==============================] - 25s 514us/step - loss: 1.0947\n",
      "Epoch 48/50\n",
      "48995/48995 [==============================] - 25s 509us/step - loss: 1.0921\n",
      "Epoch 49/50\n",
      "48995/48995 [==============================] - 24s 496us/step - loss: 1.0841\n",
      "Epoch 50/50\n",
      "48995/48995 [==============================] - 23s 472us/step - loss: 1.0702\n",
      "shall i compare thee to a summer's day?\n",
      "O that when thy self and so semmer in thee,\n",
      "Who art then best or thee worth bearty tome,\n",
      "But thou art and surel brings the peaves wither,\n",
      "Which in the beauty a prease her bage,\n",
      "Who with the there my seee my self and with detiges,\n",
      "Within the beat repourured all the conder,\n",
      "Thy all the will world bear your that were did,\n",
      "And therear you with me far my love stare,\n",
      "When thou art and sure warth, that when thou art,\n",
      "That thou shall be thou which thou art ast oftherther,\n",
      "Which should have rememy that were beauty,\n",
      "When I an thou shade of strong end, thee wret heart,\n",
      "Have for my sile thee is thy self-doth,\n",
      "And suret mand beauty a prease of thy deade.\n",
      "Then the brough thou art as thou shall be,\n",
      "  And then should that my self a doth sous deader,\n",
      "Which thy heart be steen that thou self thou mast,\n",
      "  And they she though show do thy self st and,\n",
      "Thereat me for my but that this will brand,\n",
      "But then for my self wo my pearer have I none,\n",
      "Which should that thou art as the wills and dook,\n",
      "To more what whis\n",
      "shall i compare thee to a summer's day?\n",
      "So shall that I my vorsauks you mad,\n",
      "Or an werl remurld-arouc presiy,\n",
      "I destat thee brum thiles, read me illeng,\n",
      "My vootion once the bear hen less, blon but,\n",
      "Which sen in of beautronupes boingifly'gis.\n",
      "\n",
      "\n",
      "                  60\n",
      "Seeft eyes knowhit praises desire, but for praief,\n",
      "As thou pitt am I love, which thee my doing!lled,\n",
      "Then for sweet will in the winduth on to sheet,\n",
      "I seem is praided would good soon my pioe.\n",
      "\n",
      "\n",
      "                   55\n",
      "When I would by thinbe that I was,\n",
      "  For do thy sine sund in maid,\n",
      "I dound my drobsen is sauker say I jought:\n",
      "Nit maryers shall look in new that from,\n",
      "Betine aburesto stound,\n",
      "And cen, out doth best bear and theng,\n",
      "Had for mince impiceling althe renequest near,\n",
      "And sace his bcoursing thou doth wrocce, we,\n",
      "  So thy greater toubsed boor strenst adone:\n",
      "  Thtse with life ty now how that if in thee.\n",
      "  If thy shadows will brace to there shine,\n",
      "And weel hater come of mine, shart dost tays\n",
      "Yot it to be a bwat wel'st thin\n",
      "How mo truth's face and slow,\n",
      "By nother b\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(GRU(120, input_shape=(s_len, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#train the model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam')\n",
    "model.fit(trainX, trainY, batch_size = 128, nb_epoch = 50)\n",
    "print_poem_maxprob(1000)\n",
    "print_poem_randprob(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis with variation of temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_poem_temp(length, temp, random = False):\n",
    "    if random == True:\n",
    "        start_index = np.random.randint(0, len(text)-s_len-1)\n",
    "        start_index = 50\n",
    "        sentence = text[start_index: start_index + s_len]\n",
    "    else:\n",
    "        sentence = \"shall i compare thee to a summer's day?\\n\"\n",
    "    sequence = sentence\n",
    "    for i in range(length):\n",
    "        x_pred = np.zeros((1, s_len, len(chars)))\n",
    "        for j, char in enumerate(sentence):\n",
    "            x_pred[0, j, char_to_index[char]] = 1.\n",
    "\n",
    "        predictions = np.array(model.predict(x_pred)[0])\n",
    "        for p in predictions:\n",
    "            p = np.exp(p/temp)\n",
    "        div = sum(predictions)\n",
    "        for p in predictions:\n",
    "            p = p/np.sum(predictions)\n",
    "        #predictions = preds\n",
    "        #max_index = np.argmax(predictions)\n",
    "        index = np.random.choice(len(chars), 1, p=predictions)[0]\n",
    "        next_char = index_to_char[index]\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sequence += next_char\n",
    "\n",
    "    print(sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Temperature of 1.5....\n",
      "shall i compare thee to a summer's day?\n",
      "So surmind one all more for praises will,\n",
      "Or and all here with it love, duspitit sweet:\n",
      "O 8lone thou forsique owmy thou from suy,\n",
      "Sole becuring hap yell days surot minit,\n",
      "Trey preight one, and chould how frows wall,\n",
      "When So thee over foul sispy thy doot hide,\n",
      "And to stymy gave what shish I man.\n",
      "  Unlow mire int not desarget that I foul,\n",
      "So thou gively my failtr theavince ching,\n",
      "Will not thou veity notber's lived earth,\n",
      "Whal her apceail,'ncoure her five onder sprense\n",
      "This the beak you rotere so thured so trife,\n",
      "  And sefelt thou pidque olme hane I mure,\n",
      "Nor forgater's band or fils in my bruth right,\n",
      "My anbcessed worknow, on me and greet.\n",
      "  Thou houlss the world of more,\n",
      "And all seant chnwhy being should bade,\n",
      "When I would that zence as it efail)\n",
      "Than thee that thou art confort's now, berind,\n",
      "Eves thy sleed bitterue' thine every deat:\n",
      "Thou sick your defice hass as firraters,\n",
      "I sacked her blood a bespeyenows, from for choerd\n",
      "If loge true ghill welts beauty that feer,\n",
      "Net thou goor not but\n",
      "\n",
      "\n",
      "Testing Temperature of 0.75....\n",
      "shall i compare thee to a summer's day?\n",
      "  When white hees, the least tit as thoure,\n",
      "Who thou hage with part thall over sweet (onjedtere!\n",
      "Why lives timongar, with liss, in shough:\n",
      "And to the turn of thy being sice to may\n",
      "Be of woth hy shill be to their growilg,\n",
      "Have the will wotmness rapiny lifth dus.\n",
      "Thy heart thou same of thine some bettour,\n",
      "Ond liss is his beat's self if thou procc,\n",
      "Ongh light me, by the touch a come drais ongle:\n",
      "  But me your semss thou live in my meftiess,\n",
      "Where orten with this from  had ar for my,\n",
      "Thou the'sing shade, you art me in green,\n",
      "  Love this hind destillt thing one my guent:\n",
      "\n",
      "\n",
      "                 35\n",
      "Now perfoot filleds of their garand gove,\n",
      "Shawp of you adarts with bitter's thoughts,\n",
      "In thy ing anten ane for m your sholl,\n",
      "How wor my jought regairy,\n",
      "  Singe a ducher a ar med love stoln a praise,\n",
      "Which the jucty Oaking in geastare,\n",
      "Wishs fiest majeate, thy looks now love to green,\n",
      "  If thy cragkes how she to gie, wherey,\n",
      "And lived inlunin ead this virtiors with,\n",
      "And torther best mare he beation, tha\n",
      "\n",
      "\n",
      "Testing Temperature of 0.25....\n",
      "shall i compare thee to a summer's day?\n",
      "When that that I to bounts are thrieg, breeds,\n",
      "And dear theirs with your still teme invery\n",
      "Detther mide owe-sid thas gutthar romer mike\n",
      "To cuem ald made war I badm and fild,\n",
      "  For mone on thy hand in my beloveds hower,\n",
      "And some wirt will discreast t more where thou pranser.\n",
      "Neturn outh romes all hath is with that geap by breatus,\n",
      "Not thou with thoud is thou art in my brived.\n",
      "So dat thy self doth san with thine beauty's eye,\n",
      "When you by out-faithof sure that whether runged,\n",
      "Exciss bort, that I mours unjeit,\n",
      "Painle, and, be wire's men,\n",
      "Nor day and evely hear's bebing that head,\n",
      "Usligs hath love thee heme those fanchus do the crie)ry cheed,\n",
      "  And that these both thine eyes tyreater,\n",
      "  Anr of yourate, more not thishols st lloAy,\n",
      "That thou love's pyone, wheretomenis bear,\n",
      "Tinling ond, havong, wo dut  I mmert grown,\n",
      "By conct telf had in ground is his grace.\n",
      "Then love thy love prowfsed bo my proving.\n",
      "\n",
      "\n",
      "                   11\n",
      "Whon butt on the sturn badquil haid,\n",
      "Thee by the wroccksting the pory\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Temperature of 1.5....\")\n",
    "print_poem_temp(1000, 1.5, random = False)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Testing Temperature of 0.75....\")\n",
    "print_poem_temp(1000, 0.75, random = False)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Testing Temperature of 0.25....\")\n",
    "print_poem_temp(1000, 0.25, random = False)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
